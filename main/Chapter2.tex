\chapter{RBF-FD method}

In this chapter we explain in more details the Meshless Method (MM) used within this work: the Radial Basis Function generated Finite Differences (RBF-FD) method.
To do so we show how it can be used to solve the general boundary value problem defined in chapter~\vref{chap:meshless_methods}.
From now on, in order to be able to discretize the PDE, we consider to have a disposal a set of $N$ distinct nodes, $\mathcal{X}$, defined as follow:
\begin{equation}
\label{eqn:generic_set_of_nodes_X}
	\mathcal{X} := \Set{\vec{x}_1, \dots, \vec{x} \mid \vec{x}_i \in \Omega\cup\partial\Omega, \, i=1, \dots, N}
\end{equation}
where $\Omega\cup\partial\Omega \subset \R^d$ is the physical domain of the problem, $\Omega$ and $\partial\Omega$ are respectively its open subset and boundary, and $d \in \N$ its dimension.

%TODO: Fai due parole sulcome vengono disposti i nodi sul dominio

%To do so we consider a physical domain $\Omega \cup \partial\Omega \subset \R^d$ indicating with $\Omega$ the open subset, $\partial \Omega$ its boundary and with $d \in \N$ its dimension; over the presented domain we define a boundary value problem with the same form of the one reported in~\eqref{eqn:generic_continous_PDE} and we show how RBF-FD method can be used to solve it.
%
%% Aggiungere qualche riga sull'importanza dellla distribuzione dei punti??
%As any other MM, before starting, a set of $N$ points distributed over the domain where to discretize the PDE is required. We indicate it with $\mathcal{X} := \Set{\vec{x}_1, \dots,  \vec{x}_N \mid \vec{x_i} \in \Omega\cup\partial\Omega, \, i=1, \dots, N}$. To remark also the fact that nodes are placed both inside the domain and on its boundary we partition $\mathcal{X}$ in $\mathcal{X_I}$, the set of internal nodes, and $\mathcal{X_B}$, the set of boundary nodes, defined as:
%\begin{gather*}
%	\mathcal{X_I} := \Set{\vec{x}_1, \dots,  \vec{x}_{N_I} \mid \vec{x}_i \in \Omega, \, i=1, \dots, N_I} \\
%	\mathcal{X_B} := \Set{\vec{x}_1, \dots,  \vec{x}_{N_B} \mid \vec{x}_i \in \partial\Omega, \, i=1, \dots, N_B}
%\end{gather*}
%where $N_I$ and $N_B$ indicate respectively the number of nodes inside and on the boundary. Of course $N_I + N_B = N$ and $\mathcal{X_I} \cup \mathcal{X_B} = \mathcal{X}$.
%
%This method, like other MMs, looks for an approximated solution of problem~\eqref{eqn:generic_continous_PDE} in the form of~\eqref{eqn:general_u_discretization}.

%Radial Basis Function generated Finite Differences (RBF-FD) method, as any other MM, is used to solve approximately PDEs via scattered data interpolation. In particular, to do so, it makes use of the FD to define an approximation of partial derivatives for PDEs solution.

\section{Scattered Data Interpolation}
\label{sec:scattered_data_interpolation}

Every MM, as we have already seen, on its core, is just a way to approximate the solution of a PDE and RBF-FD method is no exception; the tool used to do so is called \emph{scattered data interpolation}. In this section we first see what scattered data interpolation is and then how it is related to PDEs solution. In the second part of the explanation we see how it is applied in general by MMs so to avoid to becoming fixated on a single implementation and losing generality; moreover this approach is still useful since it establishes all the steps that are also followed by RBF-FD.
To avoid from the very beginning any kind of ambiguity we underline that scattered data interpolation, is inherently connected to data fitting and not to PDEs approximation, consequently, it finds many other applications beyond the realm of MMs. An example can be found in~\cite{Amidror:scattered_data_interpolation_in_image_processing}.

In general the interpolation problem has the following, simple, formulation. Given:
\begin{itemize}
	\item a finite set of nodes,  $\mathcal{X} \subset \R^d$, which could be the one reported in~\eqref{eqn:generic_set_of_nodes_X} and;
	\item a set of known real values $u(\vec{x}_1), \dots, u(\vec{x}_N)$, which may be obtained from a function
\end{itemize}
we want to find a continuous function $u^{h} \colon \Omega\cup\partial\Omega\subset\R^{d} \to \R$ that satisfy:
\begin{equation}
	\label{eqn:interpolation_constraints}
	u^{h}(\vec{x_i}) = u(\vec{x}_i) \qquad  \forall \vec{x}_i \in \mathcal{X}
\end{equation}
If the locations, the nodes in $\mathcal{X}$ where the measurements $u(\vec{x}_1), \dots, u(\vec{x}_N)$ are taken, are placed on a uniform or regular grid we talk about interpolation, otherwise the process above is called \emph{scattered} data interpolation. Here the main idea is to find a function $u^h$ which is a ``good''  fit to the given data, where with ``good'' we mean a function that exactly match the given measurements at the corresponding locations.

MMs also aim to provide an approximation for an unknown function defined as a linear combination of a set of basis functions as reported in equation~\eqref{eqn:general_u_discretization}; we report here for clarity its definition:
\begin{equation}
	\label{eqn:general_u_discretization_RBF_section}
	u^{h}({x}) = \sum_{k= 1}^{N} {\alpha_k B_k(\vec{x})}
\end{equation}
and we remark that coefficients $\alpha_k$ are unknown.
Here is where the theory of scattered data interpolation is applied: it tells us that we are able to find the numerical values for $\alpha_1, \dots \alpha_N$ if we impose a number of conditions equal to the number of coefficients that we are looking for. These conditions must have a form like that shown in equation~\eqref{eqn:interpolation_constraints}. Therefore if we replace the generic meshless approximation within each of the $N$ interpolation conditions we can write the following linear system:
\begin{equation}
	\label{eqn:general_system_from_scattred_data_interpolation}
	\underbrace{
		\begin{bmatrix}
			B_1(\vec{x}_1)  & \dots		& B_N(\vec{x}_1)     \\
			\vdots					& \ddots  & \vdots					      \\
			B_1(\vec{x}_N)  & \dots		& B_N(\vec{x}_N)
	\end{bmatrix}}_{\boldsymbol{B}}
	\begin{bmatrix}
		\alpha_1 \\
		\vdots		\\
		\alpha_N
	\end{bmatrix}
	=
	\begin{bmatrix}
		u(\vec{x}_1)  \\
		\vdots				\\
		u(\vec{x}_N)
	\end{bmatrix}
\end{equation}
that, once solved, provides us the desired coefficients that uniquely define $ u^h $ given the set of basis functions $ \Set{B_1, \dots, B_N} $.
We would like to stress the fact that, however, the right hand side vector is made up of values of the unknown exact solution of problem~\eqref{eqn:generic_continous_PDE}.

During the solution of problem~\eqref{eqn:generic_continous_PDE} the following constraints are enforced instead:
\begin{equation}
	\begin{aligned}
		\mathcal{L} u^h(\vec{x}_j) = f(\vec{x}_j) \quad & \text{if $\vec{x}_j \in \Omega$}  \\
		u^h(\vec{x}_j) = g(\vec{x}_j) 						    \quad & \text{if $\vec{x}_j \in \partial\Omega$}
	\end{aligned}
\end{equation}
and if we arrange the nodes such that the first $N_I$ nodes belongs to $\Omega$ and the last $N_B$ to $\partial\Omega$, we can find the values of the approximated solution at the points in $\mathcal{X}$ by solving:
\begin{equation}
	\begin{bmatrix}
		c_{1,1} 		& 	\dots 		& c_{1,N_I}  \\
		\vdots			& \ddots	& \vdots		\\
		c_{N_I,1} & \dots		& c_{N_I,N_I}
	\end{bmatrix}
	\begin{bmatrix}
		u^h(\vec{x}_1)  \\
		\vdots					\\
		u^h(\vec{x}_{N_I})
	\end{bmatrix}
	=
	\boldsymbol{f} -
	\begin{bmatrix}
		c_{1,N_I+1} 		& 	\dots 		& c_{1,N_B}  \\
		\vdots			& \ddots	& \vdots		\\
		c_{N_I,N_I+1} & \dots		& c_{N_I,N_B}
	\end{bmatrix}
	\boldsymbol{g}
\end{equation}
where $\boldsymbol{f} = [f(\vec{x}_1) \dots f(\vec{x}_{N_I})]^T$ and $\boldsymbol{g} = [g(\vec{x}_{N_I+1}) \dots g(\vec{x}_{N_B})]^T$, and the coefficient matrix $\boldsymbol{C}$ is found as the solution of:
\begin{equation}
	\label{eqn:generic_discretized_PDE_by_MMs}
	\begin{bmatrix}
		c_{1,1} 		& 	\dots 		& c_{N_I,1}  \\
		\vdots			& \ddots	& \vdots		\\
		c_{1,N} & \dots		& c_{N_I,N}
	\end{bmatrix}
	=
	\boldsymbol{B}^{-T}
	\begin{bmatrix}
		\mathcal{L} B_1(\vec{x}_1)  & \dots		& \mathcal{L} B_1(\vec{x}_{N_I})     \\
		\vdots												& \ddots  & \vdots					      								  \\
		\mathcal{L} B_N(\vec{x}_1)  & \dots		& \mathcal{L} B_N(\vec{x}_{N_I})
	\end{bmatrix}
\end{equation}
Analyzing each row $\boldsymbol{c}_i = [c_{i,1}, \dots c_{i,N}]$ of matrix $\boldsymbol{C}$ we can notice that are computed solving the following linear systems:
\begin{equation}
	\boldsymbol{B}^T c_i = 
	\begin{bmatrix}
		\mathcal{L} B_1(\vec{x_i})  \\
		\vdots											  \\
		\mathcal{L} B_N(\vec{x_i})
	\end{bmatrix}
	\qquad i=1, \dots, N_I
\end{equation}
which are closely related to the ones obtained from scattered data interpolation reported in~\eqref{eqn:general_system_from_scattred_data_interpolation} due to the presence of the same matrix $\boldsymbol{B}$. We conclude by commenting that equation~\eqref{eqn:generic_discretized_PDE_by_MMs} with matrix $\boldsymbol{B}$ defined as in~\eqref{eqn:general_system_from_scattred_data_interpolation} holds true only in case of Dirichlet boundary conditions, otherwise $\boldsymbol{B}$ would take on a different form.



\section{The Mairhuber-Curtis theorem}
\label{sec:Mairhubert-Curtis}

From the previous discussion, in particular from equation~\eqref{eqn:generic_discretized_PDE_by_MMs}, it can be noticed that matrix $\boldsymbol{B}$ has to be non singular in order to be able to solve the linear system associated to the boundary value problem, and this must hold for each node placement $\mathcal{X}$ (to be read as every possible discretization of the problem domain) as long nodes are distinct. This property of $\boldsymbol{B}$ turns out to be dependent on the choice of the particular set of basis functions: for example if we assume $B_k(\vec{x})\in\Pi_P^d$ and $\Set{B_1(\vec{x}), \dots, B_N(\vec{x})}$ to be a polynomial basis of the space $\Pi_P^d$ of polynomials of degree at most $P$ in $\R^d$, then we are not able to guarantee that $\vec{B}$ is invertible for $d>1$.

This issue is explained in more detail by the Mairhuber-Curtis theorem~\cite{Mairhuber:interpolation_basis_problem}; when dealing with the multidimensional case it is possible to continuously move two nodes along a closed path $P$, that does not interfere with any other node in $\mathcal{X}$, such that they end up by interchanging their original positions without one crossing the path of the other. In the event that these two are the only nodes of $\mathcal{X}$ that are moved, $\boldsymbol{B}$ ends up with two rows exchanged leading to a change in the sign of its determinant, and, since the determinant is a continuous function, this means that there is a moment when the latter vanishes making the matrix singular.

The inconvenience arises from the fact that the set of basis functions is independent from the node position and could be solved by simply choosing a basis that is function of nodes position. By doing so we no more fall in the case of the Mairhuber-Curtis theorem since whenever we move nodes also the base itself changes and if two nodes switches their positions not only their respective rows in $\boldsymbol{B}$ are switched, but also their columns, forcing the determinant not to change in sign.

%TODO: Say something about the suitable node generation algorithm and then tells about polynomial augmentation and why it is required (see Riccardo thesis?). Once you've done that you can continue and start talking about RBF.



\section{Radial Basis Functions} \label{subsec:radial_basis_functions}
Up to now, when talking about the approximated solution of the PDE, $u^h$, we have not specified the type of basis functions which define it. However these must be done in order to be able to find the coefficients $\alpha_k$ in equation~\eqref{eqn:general_u_discretization_RBF_section} and thus its numerical values; furthermore it would be appropriate to select a set of functions that allow the avoidance of the aforementioned Mairhuber-Curtis' theorem case. In this section we will define the ones used by the RBF-FD method: the Radial Basis Functions (RBFs).

RBFs are defined as:
\begin{equation}
	\label{eq:RBF_definition}
	\Phi(\vec{x}, \vec{x}_k) = \varphi(\norm{\vec{x} - \vec{x}_k}_2)
\end{equation}
where $\vec{x}_k \in \R^d$ is a given and known point , $\norm{\cdot}_2$ is the euclidean distance and $\varphi\colon \R \to \R$, named \emph{basic} function, is a (univariate) function which takes the radius $r_k = \norm{\vec{x} - \vec{x}_k}_2$ as input and it is used as generator for all the (multivariate) \emph{basis} functions associated to different $\vec{x}_k$. In general different basic functions $\varphi$ can be used, some of them are reported in table~\ref{tab:basic_functions}. Sometimes the notations $\Phi(\vec{x} - \vec{x}_k)$ or $\Phi_k({\vec{x}})$ are also used instead of $\Phi(\cdot, \vec{x}_k)$.

\begin{table}
	\caption{Examples of basic functions where $r$ is a real number greater than or equal to zero, and $\epsilon$, called shape factor, is a suitable parameter}
	\label{tab:basic_functions}
	\centering
	\begin{tabular}{cc}
		\toprule
		Name										&  $\varphi(r)$																	\\
		\midrule
		Multiquadratic					 			&  $\sqrt{1+(\epsilon r)^2}$													\\
		Inverse multiquadratic	 					&  $\big(  \sqrt{1 + (\epsilon r)^2}  \big)^{-1}$  								\\
		Thin plate splines			   				&  $r^{2l} \log l, \, l\in\N$													\\
		Gaussian							    	&  $e^{- (\epsilon r)^2}$												  		\\
		Polyharmonics								&  $r^{2l + 1}, \, l\in\N$														\\
		\bottomrule
	\end{tabular}
\end{table}

To further clarify the RBFs name we can notice that they are called:
\begin{description}
	\item[Radial] since the value of each $\Phi(\cdot, \vec{x}_k)$ at each point $\vec{x}$ depends only on the distance between that point and $\vec{x}_k$ through $\norm{\cdot}_2$, they satisfy radial symmetry, i.e. $\Phi(\vec{x}_i, \vec{x}_j) = \Phi(\vec{x}_j, \vec{x}_i)$ for any $\vec{x}_i, \vec{x}_j \in \R^d$;
	\item[Basis] since in case of a set of $N$ distinct nodes in $\R^d$, as can be $\mathcal{X}$ given in~\eqref{eqn:generic_set_of_nodes_X}, the set of radial functions $\Phi_k{(\vec{x})}$ with $k=1, \dots, N$ associated to each point of the set, form a basis for the space of functions:
	\[
	F_{\Phi} := \Set{ \sum_{k=1}^{N} \alpha_k \Phi_k{(\vec{x})},  \quad \alpha_k \in\R, \, \vec{x}_k \in \mathcal{X}}
	\]
\end{description}

In case of RBF-FD method the implementation of scattered data interpolation and the solution of governing equation remain the same as explained in the previous section and leads to a symmetric matrix $\boldsymbol{B}$ defined as:
\begin{equation}
	\boldsymbol{B} =
	\begin{bmatrix}
		\Phi(\vec{x}_1, \vec{x}_1)  & 	\dots 	& 	\Phi(\vec{x}_1, \vec{x}_N)  \\
		\vdots											& \ddots &  \vdots											  \\
		\Phi(\vec{x}_N, \vec{x}_1)  & 	\dots 	& 	\Phi(\vec{x}_N, \vec{x}_N)
	\end{bmatrix}
\end{equation}

We conclude by observing that this functions are particularly convenient since they depend on nodes position through $\vec{x}_k$, thus they allow to avoid the non-invertibility of matrix $\vec{B}$ in case of singular node arrangements (Mairhuber-Curtis theorem).



\section{Polynomial augmentation}

Setting aside the issue of the invertibility of matrix $\boldsymbol{B}$ in case of particular nodes arrangement discussed in previous subsection, we should also take into account  the accuracy of the interpolation that we are able to achieve, which also depends upon the type of functions that we are supposed to approximate.
Indeed RBFs approximation schemes alone are not able to exactly interpolate (i.e. with an accuracy only depending on roud-off errors) constant, linear or higher degree polynomials fields. This is an issue in different important engineering applications such as modeling of constant strain in elastic bodies and steady temperature fields in differentially heated walls~\cite{Zamolo:phd_thesis}.

To overcome this limitation, a polynomial augmentation of degree $P$ is required, leading to the overall formulation for the RBF interpolant:
\begin{equation}
	\label{eq:RBF_interpolator_plus_polynomial_augmentation}
	u^h(\vec{x}) = \sum_{j=1}^{N} \alpha_j \Phi_k(\vec{x}) + \sum_{k=1}^{M} \beta_k p_k(\vec{x})
\end{equation}
where $M=\binom{P+D}{D}$ is the number of polynomial basis functions with degree $P \le D$, $\Set{p_1(\vec{x}) \dots p_M({\vec{x}})}$ is a complete polynomial basis of $\Pi_P^d$ and $\beta_j$ are the corresponding coefficients. An example of polynomial basis for polynomials of degree $P=1$ in $2D$ has the following $M=3$ elements: $p_1(x,y) = 1$, $p_2(x,y) = x$, $p_3(x,y) = y$.

We must also note that using an interpolant with the introduced polynomial augmentation leads to an underdetermined system in~\eqref{eqn:general_system_from_scattred_data_interpolation}. In order to obtain a square $\vec{B}$ and thus having a solvable system, the following orthogonality conditions have to be imposed:
\begin{equation}
	\label{eqn:orthogonality_conditions_for_square_B_in_case_of_poly_augmentation}
	\sum_{i=1}^{N} \alpha_i p_k(\vec{x}_i) = 0, \qquad i=1, \dots, M
\end{equation}
The coefficients of $u^h$, which are now composed not only by $\vec{\alpha} = [\alpha_1 \dots \alpha_N]$, but also by $\vec{\beta} = [\beta_1 \dots \beta_M]$, can then be found by solving the following system:
\begin{equation}
\label{eqn:general_system_from_scattred_data_interpolation_poly_augmentation}
\underbrace{
\begin{bmatrix}
	\vec{B}  & \vec{P}  \\
	\vec{P}^T  & \vec{0}
\end{bmatrix}
}_{\vec{M}}
\begin{bmatrix}
	\vec{\alpha}  \\
	\vec{\beta}
\end{bmatrix}
=
\begin{bmatrix}
	\vec{u}  \\
	\vec{0}
\end{bmatrix}
\end{equation}
where:
\begin{equation}
	\begin{aligned}
		\vec{P} & =
		\begin{bmatrix}
			p_0(\vec{x}_1)  & \dots     & p_M(\vec{x}_1)  \\
			\vdots					 & \ddots  & \vdots						\\
			p_0(\vec{x}_N)  & \dots     & p_M(\vec{x}_N)  \\
		\end{bmatrix}  \\
		\vec{u} & = [u(\vec{x}_1). \dots u(\vec{x}_N)]
	\end{aligned} 
\end{equation}
From its formulation is easy to understand that the system above is simply the composition of the system in equation~\eqref{eqn:general_system_from_scattred_data_interpolation}, in the first row, with constraints~\eqref{eqn:orthogonality_conditions_for_square_B_in_case_of_poly_augmentation} written in compact form, in the second row.

In practice the addition of polynomial basis to the RBF interpolant let us perfectly fit $u(\vec{x})$ not only on collocation points where we have $u^h(\vec{x}_i)  = u(\vec{x}_i)$, but also across the rest of the domain provided that data $u(\vec{x}_1) \dots u(\vec{x}_N)$ come from a polynomial of total degree less than or equal to $P$.  Nevertheless this procedure has as a side effect since not all set of nodes $\mathcal{X}$ nor all basic functions $\varphi$ leads to a well-posed RBF interpolation with a non singular matrix $M$. We will discuss this limitation in more detail in the next section.



\section{Problem solution}

At the beginning of section~\ref{sec:Mairhubert-Curtis} we mentioned that the system in equation~\eqref{eqn:generic_discretized_PDE_by_MMs} might not be solvable in case of a combination of non-point-dependent basis functions and particular nodes arrangements, but, up to now, we did not discuss in general in which cases the interpolation problem is solvable. In this section we will address this shortcoming.

We start by recalling that, in case of pure RBF interpolant, the system that we aim to solve has the following compact form:
\begin{equation}
	\label{eqn:general_system_from_scattred_data_interpolation_compact_form}
	\vec{B} \vec{\alpha} = \vec{u}
\end{equation}
where $\vec{B}$ is a symmetric matrix.
This means that the above system require a positive definite $\vec{B}$ in order to be solved and this property depends on the choice of the basic function $\varphi$ used to define the RBFs. By definition only \emph{strictly} positive definite~\cite{Fasshauer:details_on_basic_functions} $\varphi$ are associated to a positive definite $\vec{B}$ and, this, restrict our choices: of those shown in table~\vref{tab:basic_functions} only the Inverse Multiquadratic satisfy this requirement. The same attributes of $\varphi$ are also inherited by the associated RBFs $\Phi_k$

However in previous section we have seen that, beyond the solvability issue, a polynomial augmentation of degree $P$ is beneficial for the accuracy of the interpolant $u^h$. This means that the system that we have to solve, in general, is no more in the form shown in equation~\eqref{eqn:general_system_from_scattred_data_interpolation_compact_form}, but rather in the following:
\begin{equation}
\label{eqn:general_system_from_scattred_data_interpolation_poly_augmentation_compact_form}
\vec{M}
\begin{bmatrix}
	\vec{\alpha}  \\  \vec{\beta}
\end{bmatrix} = 
\begin{bmatrix}
	\vec{u}  \\  \vec{0}
\end{bmatrix}
\end{equation}
In this case the matrix that has to be positive definite would be $\vec{M}$, which is symmetric as well, that we recall being defined as:
\begin{equation}
\label{eqn:global_M_definition}
\vec{M} =
\begin{bmatrix}
	\vec{B}    &  \vec{P}  \\
	\vec{P}^T  &  \vec{0}
\end{bmatrix}
\end{equation}
To be such the following conditions have to be met~\cite{Miotti:phd_thesis}:
\begin{enumerate}
	\item basic function $\varphi$ has to be \emph{strictly conditionally} positive definite function of order $P$~\cite{Fasshauer:details_on_basic_functions};  \label{enum:condition_on_basic_functions_for_solvability}
	\item matrix $\vec{P}$ has to be full-rank.  \label{enum:condition_on_P_for_solvability}
\end{enumerate}
Condition~\ref{enum:condition_on_basic_functions_for_solvability} allows a greater freedom on the choice of the basic function, compared to the pure RBF interpolant: in fact, the set of strictly conditionally positive definite functions of order $P$ is a superset of strictly positive definite functions. These functions are defined as those that require a polynomial augmentation of order at least $P-1$ in order to give a non singular $\vec{M}$. Strictly conditionally positive definite functions of order $P$ are also strictly conditionally positive definite of any higher order. This means that, in case $u^h$ include a polynomial augmentation of order $1$, we can also use Multiquadratic, Thin plate splines with $l=0$ and Polyharmonics with $l=1$ (refer again to table~\ref{tab:basic_functions} for their definitions) as basic funtions as these are strictly conditionally definite of order 1. At this point someone might consider increasing the degree $P$ of the polynomial augmentation up to the theoretical limit for the size of matrix $\vec{P}$, i.e. $M=N$, in order to increase the accuracy of the interpolant, but doing so results in ill-conditioning and singularity issues related to $\vec{P}$ that, since non-singularity of $\vec{P}$ is a necessary condition for the non singularity of $\vec{M}$, also affect $\vec{M}$.

This is why condition~\ref{enum:condition_on_P_for_solvability} is also required. It can be shown that to have a full-rank $\vec{P}$ the set $\mathcal{X}$, containing the nodes respect to which we are carrying out the interpolation, must be $P$-unisolvent~\cite{Fasshauer:details_on_basic_functions}, where $P$ is the degree of the polynomial augmentation. This dependency of $\vec{P}$'s rank on the node locations should not surprise since the matrix columns consist on the elements of the polynomial basis evaluated at the different points in $\mathcal{X}$, and they are required to be linearly independent.

Given the node generation technique used in this work a safe rule for a stable implementation (in the sense of a well-posed interpolation probelm) of the polynomial augmentation is to respect the inequality $2M \le N$ where $M$ is the number of terms in the polynomial basis and $N$ is the number of nodes in $\mathcal{X}$.



\section{RBF-FD}

In this section we are going to discuss in detail how RBF-FD meshless method is used to solve Partial Differential Equations (PDEs). As already briefly mentioned in section~\vref{sec:scattered_data_interpolation} interpolation of scattered data is the key to do so: in particular it is used to obtain a finite-difference-like discretization of the differential operator~\cite{Miotti:RBF_in_depth,Zamolo:RBF-FD_with_Neumann}


\subsection{Finite difference method}

%TODO: Cita come avviene l'approssimazione (secondo highlights del libro sulle FD) [Prima di quesa frase, ma non nnecessariamente in questa subsection]
Before tackling the approximation of PDEs solution, we first consider the more basic task of approximating the derivatives of a known function by Finite Difference (FD) formulas based only on values of the function itself at discrete points. Given $u$, in the simplest case a function of one variable assumed to be sufficiently smooth, we want to approximate its derivatives at a given point $\overline{x}$ relying solely on its values at a finite number of points close to $\overline{x}$. In general its $k$-th derivative is approximated by the following FD formula:
\begin{equation}
	\frac{d^k u}{dx^k}\bigg|_{x=\overline{x}} = u^{(k)}(\overline{x}) \approx \sum_{i=1}^{n} c_{i}^{k} u(x_i)
\end{equation}
where $\Set{u(x_1), \dots, u(x_n)}$ are the function's samples and $\Set{c_{1}^{k}, \dots, c_{n}^{k}}$, which can be computed in different ways such as the method of undetermined coefficients or via polynomial interpolation~\cite{LeVeque:FD_book}, are called FD weights.

To give a concrete example we could approximate $u'(\overline{x})$ with the following one-sided approximations:
\begin{subequations}
	\begin{align}
		D_+ u(\overline{x}) & = \frac{u(\overline{x}+h) - u(\overline{x})}{h}  \label{eqn:forward_FD_approx}\\
		D_- u(\overline{x}) & = \frac{u(\overline{x}) - u(\overline{x}-h)}{h}  \label{eqn:backward_FD_approx}
	\end{align}
\end{subequations}
for some value of $ h$. This is motivated by the standard definition of the derivative as the limiting value of this expression as $h \to 0$. In these cases the same FD weights, $\Set{1/h, - 1/h}$, are associated to function values coming from different discrete points: $\Set{u(\overline{x}+h), u(\overline{x})}$ for equation~\eqref{eqn:forward_FD_approx} and $\Set{u(\overline{x}), u(\overline{x}-h)}$ for~\eqref{eqn:backward_FD_approx}. Another possibility is to use the centered approximation:
\begin{equation}
	D_0 u(\overline{x}) = \frac{u(\overline{x}+h) - u(\overline{x}-h)}{h} = \frac{1}{2} \bigl( D_+ u(\overline{x}) + D_- u(\overline{x}) \bigr)
\end{equation}

To derive approximations to higher order derivatives, besides the two method mentioned above, is also possible to repeatedly apply first order differences. Just as the second order derivatives is the derivative of $u'$, we can view $D^2u(\overline{x})$, the second order derivative approximant, as being a finite difference of first differences: $D^2 u(\overline{x}) = D_+ D_- u(\overline{x})$ or $D^2 u(\overline{x}) = D_- D_+ u(\overline{x})$. If we use a step size $h/2$ in each centered approximation to the first derivative we could also define $D^2 u(\overline{x})$ as a centeered difference of centered differences and obtain:
\begin{equation}
	D^2 u(\overline{x}) = \frac{1}{h} \Biggl( \biggl( \frac{u(\overline{x}+h) - u(\overline{x})}{h} \biggr) - \biggl( \frac{u(\overline{x}) - u(\overline{x}-h)}{h} \biggr) \Biggr)
\end{equation}
where FD weights $\Set{1/h^2, -2/h^2, 1/h^2}$ are associated to $\Set{u(\overline{x}+h), u(\overline{x}), u(\overline{x}-h)}$


























