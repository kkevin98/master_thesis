\chapter{Automatic Differentiation (AD) and adjoint method in optimization}
\label{chap:adjoint_method_chapter}

In this chapter we are going to discuss Automatic Differentiation (AD) and the adjoint method which are two techniques that allows to compute efficiently the gradient of a function which depends on many parameters. We also see how they can be used to improve the efficiency of gradient-based optimization algorithms.

To do so we introduce to the possible techniques that allows to compute derivatives with a particular focus on Automatic Differentiation (AD), highlighting its advantages and its two main implementations.

Subsequently, we will provide a brief overview of design optimization problems, in particular of those related to systems which are numerically described by RBF-FD methods.

Finally, we conclude the chapter explaining in detail the application of the AD and the adjoint method to $1$D and $3$D RBF-FD based design optimization problems.

\section{Automatic Differentiation (AD)}

In general there exists different ways to compute derivatives using a computer program, these are:
\begin{description}
	\item[Manual Differentiation] In this case \emph{analytical} derivatives are computed by hand and then are plugged into standard optimization procedures such as gradient descend. Of course doing so is time consuming and prone to error.
	
	\item[Numerical Differentiation] In this case finite difference methods, as the ones reported in subsection~\vref{subsec:finite_difference_methods}, are used to approximate the derivatives \emph{values}. Easy to implement it has the disadvantage to be inaccurate due to round-off and truncation errors~\cite{LeVeque:FD_book}.

	\item[Symbolic Differentiation] This case addresses the weakness of both manual and numerical differentiation. \emph{Analytical} derivative expressions are automatically obtained by modern computer algebra systems such as Mathematica\footnote{Wolfram Research, Inc.'s proprietary software for technical computing. See~\url{https://www.wolfram.com/mathematica/} for more informations.} or SymPy\footnote{Python open source library for symbolic mathematics. The project can be found at~\url{https://www.sympy.org/en/index.html}.}. Unfortunately, often, the outcomes are plagued with the problem of ``expression swell'' which means that the resulting expressions are large, complex and cryptic. Furthermore it can be applied only to models defined in closed-form.
	
	\item[Automatic Differentiation] This last case refers to a family of techniques that compute derivative \emph{values} (in contrast with symbolic differentiation) by using symbolic rules of differentiation (but keeping track of derivative values as opposed to the resulting expressions) through accumulation of values during code execution. Thanks to this, Automatic Differentiation (AD), can be applied to code involving branches, loops and recursion as opposed to symbolic differentiation~\cite{Baydin:AD_survey}. The mix between symbolic and numerical differentiation gives to these methodologies an hybrid nature.
\end{description}

For the remaining of this section we will explain the details of Automatic Differentiation (AD), to do so we will look at its two (main) implementations:
\begin{itemize}
	\item \emph{forward mode}, also known as tangent linear mode;
	\item \emph{reverse mode} also known as cotangent linear mode or \emph{adjoint} mode.
\end{itemize}
To explain how each of the two modes works, we show how they are applied to a function $\tilde{f} \colon \R^2 \to \R^2$ defined as:
\begin{equation}
	\label{eqn:example_function_for_AD}
	\tilde{f}(x_1, x_2) = \begin{bmatrix}
					\tilde{f}_1(x_1, x_2)  &  \tilde{f}_2(x_1, x_2)
				  \end{bmatrix}
\end{equation}
with $\tilde{f}_1(x_1, x_2) = x_1 x_2 + \cos x_1$ and $\tilde{f}_2(x_1, x_2) = x_2^3 + \ln x_1 - x_2$.

\smallskip
Before entering into the detail, we notice that every function $f \colon \R^n \to \R^m$ can be rewritten as a computational graph. A computational graph is a direct graph whose nodes correspond to operations and each operation can feed its output into other operations; once the graph is fed with some variables each node become automatically function of those variables. The usual operations considered as nodes are: binary arithmetic operations, the unary sign switch and transcendental functions such as exponential, logarithm and trigonometric functions.
Creating a computational graph for a function becomes easy by indicating with:
\begin{itemize}
	\item $v_{i-n}=x_i$, $i=1, \dots, n$ the input variables;
	\item $v_i$, $i=1, \dots, l$ the intermediate variable;
	\item $y_{m-i}=v_{l-i}$, $i=m-1, \dots, 0$ the output variables.
\end{itemize}
To better identify the relationship between the elementary operations which constitute a function it is helpful to create an \emph{evaluation trace} for the function itself.
Left-hand side of table~\ref{tab:forward_AD_example} and figure~\ref{fig:example_of_computational_graph} contains respectively the evaluation trace and the computational graph for function $\tilde{f}$.

\begin{figure}
\centering
\begin{tikzpicture}
	[circle, inner sep=0pt, minimum size=10mm,
	operation/.style={draw},
	input/.style={draw=none},
	output/.style={draw=none}, -latex, auto, semithick]
	
	
	\node[operation] (v_minus_one) 				   								   {$v_{-1}$};
	\node[operation] (v_zero)	   [below=of v_minus_one, yshift=-20mm] 		   {$v_0$};
	\node[input]	 (x_one)	   [left=of v_minus_one] 						   {$x_1$};
	\node[input]	 (x_two)	   [left=of v_zero] 							   {$x_2$};
	
	\node[operation] (v_one)       [right=of v_minus_one, xshift=5mm, yshift=10mm] {$v_1$};
	\node[operation] (v_two)       [below=of v_one] 							   {$v_2$};
	\node[operation] (v_three)     [below=of v_two] 							   {$v_3$};
	\node[operation] (v_four)      [below=of v_three] 							   {$v_4$};
	
	\node[operation] (v_six)       [right=of v_four, xshift=5mm] 				   {$v_6$};
	
	\node[operation] (v_seven)     [right=of v_six, xshift=5mm, yshift=10mm] 	   {$v_7$};
	\node[operation] (v_five)      [above=of v_seven, yshift=20mm] 			   	   {$v_5$};
	\node[output]	 (f_one)	   [right=of v_five] 							   {$f_1(x_1,x_2)$};
	\node[output]	 (f_two)	   [right=of v_seven] 							   {$f_2(x_1,x_2)$};
	
	
	\draw [->] (x_one)		 to (v_minus_one);
	\draw [->] (x_two)		 to (v_zero);
	\draw [->] (v_minus_one) to (v_one);
	\draw [->] (v_minus_one) to (v_two);
	\draw [->] (v_minus_one) to (v_four);
	\draw [->] (v_zero) 	 to (v_two);
	\draw [->] (v_zero) 	 to (v_three);
	\draw [->] (v_zero) 	 to (v_seven);
	\draw [->] (v_one)		 to (v_five);
	\draw [->] (v_two)		 to (v_five);
	\draw [->] (v_three)	 to (v_six);
	\draw [->] (v_four)	 	 to (v_six);
	\draw [->] (v_six)		 to (v_seven);
	\draw [->] (v_five)		 to (f_one);
	\draw [->] (v_seven)	 to (f_two);
\end{tikzpicture}
\caption{Computational graph of function $\tilde{f}(x_1, x_2) = \Bigl[
		\tilde{f}_1(x_1, x_2) \,\, \tilde{f}_2(x_1, x_2) \Bigr]$. Definitions of intermediate variables $ v_{-1}, \dots, v_7$ can be found in table~\ref{tab:forward_AD_example} or table~\ref{tab:reverse_AD_example}}
\label{fig:example_of_computational_graph}
\end{figure}

\begin{table}
	\centering
	\begin{tabular}{cll}
		\toprule
		\multicolumn{3}{l}{\text{Forward Primal Trace}}  	 \\
		$v_{-1}$ &  $=x_1$ 				  & $=2$  			 \\
		$v_0$	 &  $=x_2$ 				  & $=3$  			 \\
		\midrule
		$v_1$	 &  $=\cos v_{-1}$  	  & $=\cos 2$      	 \\
		$v_2$	 &  $=v_{-1}  v_0$  	  & $=2 \cdot 3$  	 \\
		$v_3$	 &  $=v_0^3$  			  & $=3^3$	  	  	 \\
		$v_4$	 &  $=\ln v_{-1}$  	  	  & $=\ln 2$  	  	 \\
		$v_5$	 &  $=v_2 + v_1 \quad$    & $=6-0.416$   	 \\
		$v_6$	 &  $=v_3 + v_4$  		  & $=27+0.693$  	 \\
		$v_7$	 &  $=v_6 - v_0$  		  & $=27.693-3$  	 \\
		\midrule
		$y_1$	 &  $=v_5$				  & $=5.584$		 \\
		$y_2$	 &  $=v_7$				  & $=24.693$		 \\
		\bottomrule
	\end{tabular}
	%	\begingroup\setlength{\fboxsep}{0pt}
	%	\colorbox{lightgray}{
		\begin{tabular}{cll}
			\toprule
			\multicolumn{3}{l}{\text{Forward Tangent (Derivative) Trace}}  							 \\
			$v'_{-1}$ 		 &  $=x'_1$						  			& $=1$  					 \\
			$v'_0$	  		 &  $=x'_2$ 					  			& $=0$  					 \\
			\midrule
			$v_1'$	  		 &  $=-v_{-1}'\sin v_{-1}$  	  			& $= -1 \cdot \sin 2$      	 \\
			$v_2'$	  		 &  $=v_{-1}'  v_0 + v_{-1}  v_0' \quad$	& $=1 \cdot 3 + 2 \cdot 0$ 	 \\
			$v_3'$	  		 &  $=3  v_o^2  v_0'$  						& $=3 \cdot 2^2 \cdot 0$  	 \\
			$v_4'$	  		 &  $=v_{-1}' / v_{-1}$  			  		& $=1/2$	  	  	 		 \\
			$v_5'$	  		 &  $=v_2' + v_1'$  		  				& $=3 - 0.909$   	 		 \\
			$v_6'$	  		 &  $=v_3' + v_4'$  		  				& $=0+0.5$  	  			 \\
			$v_7'$	  		 &  $=v_6' - v_0'$  		  				& $=0.5 - 0$  				 \\
			\midrule
			$\mathbf{y'_1}$	 &  $\mathbf{=v_5'}$				  		& $\mathbf{=2.091}$			 \\
			$\mathbf{y'_2}$	 &  $\mathbf{=v_7'}$				  		& $\mathbf{=0.5}$			 \\
			\bottomrule
		\end{tabular}
		%	} \endgroup
	\caption{Forward mode AD example to evaluate the derivatives $\frac{\partial y_1}{\partial x_1}$ and $\frac{\partial y_2}{\partial x_1}$ of $\tilde{f}(x_1, x_2)$ at $[x_1, x_2] = [2,3]$. $x_1'$ and $x_2'$ are respectively set to $1$ and $0$ in order to derive only respect $x_1$. On the left is reported the forward evaluation trace, on the right the tangent one}
	\label{tab:forward_AD_example}
\end{table}

Using the aforementioned representations we see that every function ultimately is a composition of elementary operations. This also means that its numerical derivatives can be computed by combining all the numerical derivatives of the constituent operations through the \emph{chain rule}: this, is the main idea of Automatic Differentiation.

%TODO: 1) Talk about automatic label misconception
%Before going ahead we would like to draw reader's attention on the risk of ambiguity that the term ``automatic'' in  AD generate. One might label as AD every technique that allows to compute derivatives without computing them manually (e.g. Symbolic Differentiation), but in technical terms AD is used to indicate only those techniques that compute derivatives through accumulation of values during code execution to generate numerical numerical derivative evaluations rather than derivative expression.


\subsection{Forward mode}
\label{subsec:forward_mode_AD}

In order to compute the derivative of the function $\tilde{f}$, reported in~\eqref{eqn:example_function_for_AD}, with respect to $x_1$ we start by considering the evaluation trace on the left-hand side of table~\ref{tab:forward_AD_example} and we associate to each intermediate variable $ v_i$ the derivative:
\[
	v_i' = \frac{\partial v_i}{\partial x_1}
\]
Here the variable with respect to which we differentiate remain the same independently on $i$.
Moving from top to bottom in the forward primal trace the corresponding tangent (derivative) trace, presented on the right-hand side of table~\ref{tab:forward_AD_example}, is generated by applying the chain rule to each encountered operation. After the primals $v_i$ are evaluated, also the corresponding tangents $v_i'$ are, again, from top to bottom, which means that tangent trace can be computed in parallel with the forward trace. Doing so gives us the desired derivatives in the final variables $v_5' = \frac{\partial y_1}{\partial x_1}$ and $v_7' = \frac{\partial y_2}{\partial x_1}$.
%TODO: Cite the fact the output is exact except for floating point appprox

\medskip
Forward mode can also be employed to evaluate the Jacobian of a generic function $f \colon \R^n \to \R^m$ at a point $\vec{x} = \vec{a}$ by performing $n$ distinct forward passes. By setting only one variable $x_i'=1$ and the others to zero we obtain:
\[
	y_j' = \frac{\partial y_j}{\partial x_i}\bigg|_{\vec{x}=\vec{a}} \qquad j=1,\dots,m.
\]
and reiterating for $i=1,\dots,n$, placing the resulting vectors side by side, we eventually obtain the desired Jacobian:
\[
\vec{J}_f =
\left.
\begin{bmatrix}
	\frac{\partial y_1}{\partial x_1} &  \dots  & \frac{\partial y_1}{\partial x_n}  \\
	\vdots							  & \ddots  & \vdots							 \\
	\frac{\partial y_m}{\partial x_n} &  \dots  & \frac{\partial y_m}{\partial x_n}
\end{bmatrix}
\right|_{\vec{x} = \vec{a}}
\]

Furthermore, by properly fine-tuning the values of the variables $x_1', \dots, x_n'$, it is possible to compute efficiently and in a matrix-free way Jacobian-vector products:
\[
\vec{J}_f \vec{r} =
\begin{bmatrix}
	\frac{\partial y_1}{\partial x_1} &  \dots  & \frac{\partial y_1}{\partial x_n}  \\
	\vdots							  & \ddots  & \vdots							 \\
	\frac{\partial y_m}{\partial x_1} &  \dots  & \frac{\partial y_m}{\partial x_n}
\end{bmatrix}
\begin{bmatrix}
	r_1		\\
	\vdots  \\
	r_n
\end{bmatrix}
\]
To accomplish this all that needs to be done is simply initializing $\vec{x}'=\big[x_1', \dots, x_n' \big]$ with $\vec{r}$; this result is particularly important in the evaluation of directional derivatives.
Before moving on, it is crucial to point out that forward mode AD:
\begin{itemize}
	\item for a function $f \colon \R \to \R^m$ allows to evaluate all its derivatives in just one forward pass, regardless of $m$;
	\item for a scalar field $f \colon \R^n \to \R$, the evaluation of its gradient $\nabla f = \Bigl[ \frac{\partial y}{\partial x_1}, \dots, \frac{\partial y}{\partial x_n} \Bigr]$ always require $n$ evaluations (since gradient is nothing more than a Jacobian of size $1 \times n$).
	%TODO: Add note of the factthat that it is not advantageous respect others methods (FD methods also scales linearly??)
\end{itemize}
This means that the here explained implementation of Automatic Differentiation (AD) has a computational cost which scales linearly with the number of function inputs. It is effective during the computation of derivative for cases $f \colon \R^n \to \R^m$ where $n \ll m$; for cases $n \gg m$ \emph{reverse mode} is more beneficial: we will discover why in the next subsections.

%TODO: 1) Aggiungi colore alla tabella di dx
%TODO: 2) Aggiungi descrizione tabelle
%TODO: 3) Aggiungi frecce alle tabelle

\subsection{Reverse mode}
\label{subsec:reverse_mode_AD}

In this case, as opposed to forward mode AD, the derivatives are propagated back from a given output: this means that the underlying mechanism is similar, yet more general, to backpropagation algorithm used in neural networks.
%A demonstration of how it works is presented in table~\ref{tab:reverse_AD_example} where we compute the sensitivities, i.e. of output $y_1 = \tilde{f}_1(x_1, x_2)$ of $\tilde{f}$ respect the inputs $x_1$ and $x_2$.

To do so, different variables, called adjoints, are associated to each variable $v_i$ of the Forward Primal Trace and each of them is defined as:
\[
\overline{v}_i = \frac{\partial y_j}{\partial v_i}
\]
which is nothing more than the sensitivity (i.e. derivative) of the output $y_j$ with respect to changes in $v_i$. It is important for the reader to note that, with this mode, the variable with respect to the derivatives are computed is not fixed as in forward mode AD. An example of reverse mode AD is presented in table~\ref{tab:reverse_AD_example} where we show how to compute the sensitivities of the output $y_1 = \tilde{f}_1(x_1, x_2)$ with respect to the inputs $x_1$ and $x_2$.

\begin{table}
	\centering
	\begin{tabular}{cll}
		\toprule
		\multicolumn{3}{l}{\text{Forward Primal Trace}}  	 \\
		$v_{-1}$ &  $=x_1$ 				  & $=2$  			 \\
		$v_0$	 &  $=x_2$ 				  & $=3$  			 \\
		\midrule
		$v_1$	 &  $=\cos v_{-1}$  	  & $=\cos 2$      	 \\
		$v_2$	 &  $=v_{-1}  v_0$  	  & $=2 \cdot 3$  	 \\
		$v_3$	 &  $=v_0^3$  			  & $=3^3$	  	  	 \\
		$v_4$	 &  $=\ln v_{-1}$  	  	  & $=\ln 2$  	  	 \\
		$v_5$	 &  $=v_2 + v_1 \quad$    & $=6-0.416$   	 \\
		$v_6$	 &  $=v_3 + v_4$  		  & $=27+0.693$  	 \\
		$v_7$	 &  $=v_6 - v_0$  		  & $=27.693-3$  	 \\
		\midrule
		$y_1$	 &  $=v_5$				  & $=5.584$		 \\
		$y_2$	 &  $=v_7$				  & $=24.693$		 \\
		\bottomrule
	\end{tabular}
	%	\begingroup\setlength{\fboxsep}{0pt}
	%	\colorbox{lightgray}{
		\begin{tabular}{clll}
			\toprule
			\multicolumn{4}{l}{\text{Reverse Adjoint (Derivative) Trace}}  							 \\[2ex]
			$\mathbf{\overline{x}_1}$  &  $\mathbf{ =\frac{\partial y_1}{\partial v_{-1}} \frac{\partial v_{-1}}{\partial x_1} }$  & $\mathbf{=\overline{v}_{-1} \cdot 1 }$  &  $=2.091$  \\[2ex]
			$\mathbf{\overline{x}_2}$  &  $\mathbf{ =\frac{\partial y_1}{\partial v_0} \frac{\partial v_0}{\partial x_2}}$  	   &  $\mathbf{ =\overline{v}_0 \cdot 1 }$   &  $=2$		  \\[2ex]
			
			\midrule
			$\overline{v}_{-1}$  &  $=\frac{\partial y_1}{\partial v_1} \frac{\partial v_1}{\partial v_{-1}} + \frac{\partial y_1}{\partial v_2} \frac{\partial v_2}{\partial v_{-1}} + \frac{\partial y_1}{\partial v_4} \frac{\partial v_4}{\partial v_{-1}}$  &  $=-\overline{v}_1 \sin (v_{-1}) + \overline{v}_2 v_0 + \overline{v}_4/v_{-1}$  &  $=2.091$  \\[2ex]
			$\overline{v}_0$	 &  $=\frac{\partial y_1}{\partial v_2} \frac{\partial v_2}{\partial v_0} + \frac{\partial y_1}{\partial v_3} \frac{\partial v_3}{\partial v_0}$  & $=\overline{v}_2 v_{-1} + 3 \overline{v}_3 v_0^2$  &  $=2$  \\[2ex]
			$\overline{v}_1$	 &  $=\frac{\partial y_1}{\partial v_5} \frac{\partial v_5}{\partial v_1}$  &  $=\overline{v}_5 \cdot 1$  &  $=1$  \\[2ex]
			$\overline{v}_2$	 &  $=\frac{\partial y_1}{\partial v_5} \frac{\partial v_5}{\partial v_2}$  &  $=\overline{v}_5 \cdot 1$  &  $=1$  \\[2ex]
			$\overline{v}_3$	 &  $=\frac{\partial y_1}{\partial v_6} \frac{\partial v_6}{\partial v_3}$  &  $=\overline{v}_6 \cdot 1$  &  $=0$  \\[2ex]
			$\overline{v}_4$	 &  $=\frac{\partial y_1}{\partial v_6} \frac{\partial v_6}{\partial v_4}$  &  $=\overline{v}_6 \cdot 1$  &  $=0$  \\[2ex]
			$\overline{v}_6$	 &  $=\frac{\partial y_1}{\partial v_7} \frac{\partial v_7}{\partial v_6}$  &  $=\overline{v}_7 \cdot 1$  &  $=0$  \\[2ex]
			
			\midrule
			$\overline{v}_5$  &  $=\overline{y}_1$	&  &  $=1$	\\
			$\overline{v}_7$  &  $=\overline{y}_2$  &  &  $=0$	\\
			\bottomrule
		\end{tabular}
		%	} \endgroup
	\caption {Reverse mode AD example with $[y_1, y_2] = \tilde{f}(x_1, x_2)$ evaluated at $[x_1, x_2] = [2,3]$. First, primal trace is evaluated (table above) and then adjoint variables are computed, from the  bottom up, in a second phase (table below). The initialization $\overline{v}_5 = \frac{\partial y_1}{\partial v_5} = \frac{\partial y_1}{\partial y_1} = \overline{y}_1 = 1$ and $\overline{v}_7 = \frac{\partial y_1}{\partial v_7} = \frac{\partial y_1}{\partial y_2} = \overline{y}_2 = 0$ is such that it allows the sensitivities to be calculated with respect the fist output}
	%	\caption{Reverse mode AD example to evaluate the derivatives $\frac{\partial y_1}{\partial x_1}$ and $\frac{\partial y_1}{\partial x_2}$ of $\tilde{f}(x_1, x_2)$ at $[x_1, x_2] = [2,3]$. $y_1'$ and $y_2'$ are respectively set to $1$ and $0$ in order to obtain the sensitivities respect $y_1$. Above is reported the forward evaluation trace, below the adjoint one}
	\label{tab:reverse_AD_example}
\end{table}

\clearpage

Since typical usage of chain rule is forward derivatives propagation, reverse mode AD might appear confusing at first sight; to make it clearer we show how the chain rule can be used to back-propagate derivatives in order to compute the contribution $\overline{v}_0$ of the change in variable $v_0$ to the change in the output $y_1$. From figure~\ref{fig:example_of_computational_graph} can be seen that the only way variable $v_0$ can affect $y_1$ is through affecting $v_2$ and $v_3$, so its contribution to the change in $y_1$ can be computed using the chain rule as follow:
\begin{equation}
	\frac{\partial y_1}{\partial v_0} = \frac{\partial y_1}{\partial v_2} \frac{\partial v_2}{\partial v_0} + \frac{\partial y_1}{\partial v_3} \frac{\partial v_3}{\partial v_0}
\end{equation}
which can be rewritten in
\begin{equation}
	\begin{split}
		\frac{\partial y_1}{\partial v_0} & = \overline{v}_2 \frac{\partial v_2}{\partial v_0} + \overline{v}_3 \frac{\partial v_3}{\partial v_0}  \\[2ex]
										  & = \overline{v}_2 v_{-1} + 3 \overline{v}_3 v_0^2
	\end{split}
\end{equation}
where quantities $\overline{v}_2$ and $\overline{v}_3$ are those that are computed first, from previous passages of the method, and derivatives $\frac{\partial v_2}{\partial v_0}$, $\frac{\partial v_3}{\partial v_0}$, can be easily computed by evaluating the result of symbolic differentiation of the elementary operations on the primals $v_i$.

%TODO: Frase vecchia per unire adjoint e AD, ma poi scartata
%Generalizing, it is possible to also rewrite $\frac{\partial y_1}{\partial v_0}$ as:
%\begin{equation}
%	\begin{split}
%		\frac{\partial y_1}{\partial v_0} & = \sum_i \frac{\partial y_1}{\partial v_i} \frac{\partial v_i}{\partial v_0}  \\[2ex]
%										  & = \sum_i \overline{v}_i \frac{\partial v_i}{\partial v_0}
%	\end{split}
%\end{equation}
%where the intermediate quantities $v_i$ are first computed.

\medskip
In light of the previous example, it is clear that, to compute derivatives, primals $v_i$ must be known along their dependencies within the computational graph. To do so two phases are required:
\begin{itemize}
	\item A \emph{forward step} where variables $v_i$ are populated and their dependencies recorded;
	\item A \emph{reverse step} where adjoints $\overline{v}_i$ are evaluated from outputs to inputs using values obtained in the first step.
\end{itemize}
We remark that once the backward pass is completed we get \emph{all} the sensitivities $\frac{\partial y_j}{\partial x_1}, \dots, \frac{\partial y_j}{\partial x_n}$ in one single pass; this means that for scalar fields $f \colon \R^n \to \R$ the computation of the gradient $\nabla f$ requires only one application of reverse mode as opposed to the $n$ passes required in case of forward mode. Nevertheless, this advantage comes at the cost of increased memory requirements which grows proportionally (in the worst case) to the number of operations in the evaluated function~\cite{Baydin:AD_survey}.

Generalizing the reasoning to functions $f \colon \R^n \to \R^m$, similarly to how it was done in previous section, we can conclude that reverse mode AD can also be used to evaluate Jacobians at a point; in particular, since each pass of reverse AD allows to compute one row of the Jacobian, its usage is advantageous when $m \ll n$ .
With regard to products between Jacobain and vector, reverse AD allows to compute efficiently vector-Jacobian products of type:
\begin{equation}
	\vec{r}^T \vec{J}_f =
	\begin{bmatrix}
		r_1 	& \dots  & r_m
	\end{bmatrix}
	\begin{bmatrix}
		\frac{\partial y_1}{\partial x_1} &  \dots  & \frac{\partial y_1}{\partial x_n}  \\
		\vdots							  & \ddots  & \vdots							 \\
		\frac{\partial y_m}{\partial x_1} &  \dots  & \frac{\partial y_m}{\partial x_n}
	\end{bmatrix}
\end{equation}
by initializing $\vec{\overline{y}}=\big[\overline{y}_1, \dots, \overline{y}_m \big]$ with $\vec{r}$.


%TODO: Valuta la notazione $\vec{\overline{y}}$ che è un po' ambigua nel caso in cui cambi output da differenziare
%TODO: Cita più spesso AD survey 

%TODO: Remove old section before final revision
%\section{Application to design optimization}
%\label{sec:application_to_design_opt}
%
%Design optimization is a topic that spans multiple disciplines, examples include, but are not limited to: wing configuration in aerospace, structural design in civil engineering, circuit design in electrical engineering, mechanism design in mechanical engineering and neural networks sizing and arches weighting in computer science.
%An example of the results that can be achieved through design optimization is shown in figure~\ref{fig:wing_opt}.
%
%\begin{figure}
%	\centering
%	\includegraphics[width=.5\textwidth]{img/wing_opt.png}
%	\caption{Example of design optimization: baseline and final design of ONERA M6 wing is shown. It can be seen how the relative pressure exerted by the air on the wing has been minimized. Figure taken from~\cite{Bombardieri:paper_of_wing_opt_image}}
%	\label{fig:wing_opt}
%\end{figure}
%
%From a formal point of view it is the process of finding the best design parameters $\vec{l} = [l_1, \dots, l_N]$ that satisfy project requirements~\cite{Matlab:design_opt}. Requirements and objectives, which are usually multiple and conflicting, are typically expressed in the form of a scalar function $J \colon \R^M\times\R^N \to \R$ named \emph{cost} or \emph{objective function}. $J$ depends on the state $\vec{u} \in \R^M$ of the problem at hand and on the design parameters $\vec{l} \in \R^N$; $\vec{u}$ is generally found as solution of a system of linear equations $\vec{Au} = \vec{b}$, which arise from the imposition of the constraints of the problem, where $\vec{A} \in \R^{M \times M}$ and $\vec{b} \in \R^M$ depends on some way on $\vec{l}$. In figure~\ref{fig:design_opt_scheme} can be found a scheme of the whole process.
%
%%This type of process is of interest since it is encountered in the most diverse fields, from computer science, where for example the proper weights of a neural network has to be found in order to minimize the error between its predicted and the desired output respect a given set of inputs, to mechanical engineering where, for example, one may want to modify the shape of an heat sink to maximize the amount of the heat transferred to the environment.
%
%\begin{figure}
%	\centering
%	\includegraphics[width=\textwidth]{img/design_opt_scheme.pdf}
%	\caption{Scheme of a design optimization process. From an initial design defined by a set of parameters $\vec{l}$ the associated state $\vec{u}$ is derived. The state in conjunction with the parameters are then used to evaluate the design through a function $J$. If the objectives are met the design is kept otherwise the whole procedure is repeated with a new set of parameters obtained by modifying the previous ones by a quantity $\Delta\vec{l}$} 
%	\label{fig:design_opt_scheme}
%\end{figure}
%
%To find the best parameters for a given project a variety of iterative approaches can be pursued during the optimization phase:
%\begin{description}
%	\item[Manual approach] where each parameter $l_1, \dots, l_N$ is adjusted one at a time. Unfortunately doing so leads to suboptimal results;
%	\item[Brute-force approach] where all possible combinations of design parameters are evaluated. However, this is time-expensive and for large-scale projects, where parameters can be hundreds, thousands, or even more, optimal parameters may not be found in reasonable time-frames;
%	\item[Gradient approach] where the gradient $\frac{dJ}{d\vec{l}}$ permits both to update all the parameters $l_1, \dots, l_N$ at the same time and to disregard most of the parameter values that are sub-optimal by considering only those suggested by its direction.
%\end{description}
%For the effectiveness of the last mentioned approach, is crucial to ensure an efficient computation of the gradient $\frac{dJ}{d\vec{l}}$ which must require as few steps as possible and be efficient even for problems involving huge number of design parameters. To meet these requirements Automatic Differentiation (AD) in its reverse-mode declination is necessary.
%%TODO: adjust the end of the "paragraph"
%
%\bigskip
%Given its advantages in design optimization we have learned that using the gradient is a good course of action. To directly evaluate $\frac{dJ}{d\vec{l}}$, in case of a particular set of design parameters defined by $\vec{l}$ at a given state $\vec{u}$, we would compute:
%\begin{equation}
%	\label{eqn:generic_gradient_design_opt}
%	\frac{dJ}{d\vec{l}}^T = \frac{\partial J}{\partial \vec{u}}^T \frac{d\vec{u}}{d\vec{l}} + \frac{\partial J}{\partial \vec{l}}^T
%\end{equation}
%%TODO: Valuta di precisare la notazione da qualce parte invece di metterla in mezzo al documento
%where notations $\frac{d}{dx}$ and $\frac{\partial}{\partial x}$ indicate the total and the partial derivatives respect the variable $x$ respectively.
%Terms $\frac{\partial J}{\partial \vec{u}}$ and $\frac{\partial J}{\partial \vec{l}}$ can be efficiently obtained by means of reverse-mode AD, as explained in subsection~\vref{subsec:reverse_mode_AD}, since $J$ is known analytically. The matrix $\frac{d\vec{u}}{d\vec{l}}$, on the other hand, requires more careful considerations.
%
%Differentiating the problem constraints $\vec{Au} = \vec{b}$ with respect to the parameters $\vec{l}$ gives:
%\begin{equation}
%	\frac{d\vec{A}}{d\vec{l}} \vec{u} + \vec{A} \frac{d\vec{u}}{d\vec{l}} = \frac{d\vec{b}}{d\vec{l}}
%\end{equation}
%which allows to obtain the sought term $\frac{d\vec{u}}{d\vec{l}}$ as:
%\begin{equation}
%	\label{eqn:state_sensitivity_respect_single_param}
%	\frac{d\vec{u}}{d\vec{l}} = \vec{A}^{-1} \left( \frac{d\vec{b}}{d\vec{l}} - \frac{d\vec{A}}{d\vec{l}}\vec{u} \right)
%\end{equation}
%We notice that $\frac{d\vec{u}}{d\vec{l}}$ is a matrix of size $M \times N$ whose $j$-th column represent the sensitivity of the state $\vec{u}$ respect to the parameter $l_j$. 
%In practice it is populated column-wise by solving equation~\eqref{eqn:state_sensitivity_respect_single_param} $N$ times when the derivative is taken with respect the $j$-th design parameter $l_j$ rather than $\vec{l}$: this gives its $j$-th column. In this process $\frac{d\vec{b}}{d\vec{l}}$ and $\frac{d\vec{A}}{d\vec{l}}$ terms can be simply obtained using AD, if $\vec{b}$ and $\vec{A}$ are known analytically, and once $\frac{d\vec{u}}{d\vec{l}}$ is computed it can be substituted in equation~\eqref{eqn:generic_gradient_design_opt} to obtain the gradient which symbolically reads as:
%\begin{equation}
%	\label{eqn:generic_gradient_design_opt_naive}
%	\frac{dJ}{d\vec{l}} = \frac{\partial J}{\partial \vec{u}}^T \left[ \vec{A}^{-1} \left( \frac{d\vec{b}}{d\vec{l}} - \frac{d\vec{A}}{d\vec{l}}\vec{u} \right) \right]  + \frac{\partial J}{\partial \vec{l}}^T
%\end{equation}
%
%However the computational costs of this naive procedure scales linearly with the number of the design parameters $N$, since $N$ solutions of linear systems are required in order to construct $\frac{d\vec{u}}{d\vec{l}}$. We finally remark that each of these inversions has the same computational cost of solving $\vec{A} \vec{u}= \vec{b}$ for $\vec{u}$ and if the solution represent the output of a particularly time consuming computation, tion is an example of, this is particularly penalizing. A more efficient gradient computation is therefore required.
%
%\smallskip
%To do so we can employ the \emph{adjoint method} which simply consist of a smarter bracketing of the equation which gives the gradient. In fact is possible to rewrite~\eqref{eqn:generic_gradient_design_opt_naive} as:
%\begin{equation}
%	\begin{aligned}
%		\frac{dJ}{d\vec{l}} & = \left[ \frac{\partial J}{\partial \vec{u}}^T \vec{A}^{-1} \right] \left( \frac{d\vec{b}}{d\vec{l}} - \frac{d\vec{A}}{d\vec{l}}\vec{u} \right) + \frac{\partial J}{\partial \vec{l}}^T  \\[2ex]
%							& = \vec{\lambda}^T \left( \frac{d\vec{b}}{d\vec{l}} - \frac{d\vec{A}}{d\vec{l}}\vec{u} \right) + \frac{\partial J}{\partial \vec{l}}^T
%	\end{aligned}
%\end{equation}
%where the vector $\vec{\lambda} \in \R^L$, whose elements are called \emph{adjoint variables}, can be found by solving:
%\begin{equation}
%	\label{eqn:generic_adjoint_system_design_opt}
%	\vec{A}^T \vec{\lambda} = \frac{\partial J}{\partial \vec{u}}
%\end{equation}
%
%By doing so is possible to solve the system in equation~\eqref{eqn:generic_adjoint_system_design_opt}, which is called \emph{adjoint problem}, only \emph{once} independently on the number $N$ of design parameters for each gradient computation: this significantly reduces the computational burden of the whole optimization process. Bear in mind that the adjoint problem has the same size as the problem defined by the constraints $\vec{A} \vec{u} = \vec{b}$ and the computational cost for their solution is the same.
%
%\bigskip
%At this point it is worth noting that the adjoint method explained above is just a particular application of reverse mode AD. To make the connections between the two methods clearer we note that the first computation of the  objective function's gradient, presented in equation~\eqref{eqn:generic_gradient_design_opt}, contains the following vector-Jacobian product:
%\begin{equation}
%	\label{eqn:vJp_design_opt}
%	\frac{\partial J}{\partial \vec{u}}^T \frac{d\vec{u}}{d\vec{l}}
%\end{equation}
%where $\frac{\partial J}{\partial \vec{u}}^T \in \R^M$ is the vector that multiply $\frac{d\vec{u}}{d\vec{l}} \in \R^{M \times N}$, the Jacobian of the state $\vec{u}$ with respect to the design parameters $\vec{l}$.
%From subsection~\ref{subsec:reverse_mode_AD} we known that AD allows to compute the resulting vector without explicitly computing the Jacobian $\frac{d\vec{u}}{d\vec{l}}$. But this is precisely what the adjoint method does through equations~(\ref{eqn:generic_gradient_design_opt} -~\ref{eqn:generic_adjoint_system_design_opt}) by first applying the chain rule to the leftmost term in the gradient formula and then parenthesizing the result in order to make the number of inversions of the matrix $\vec{A}$ independent on the number of parameters.
%The result of the whole process is then summarized by:
%\begin{equation}
%	\label{eqn:resulting_term_adjoint_method}
%	\vec{\lambda}^T \left( \frac{d\vec{b}}{d\vec{l}} - \frac{d\vec{A}}{d\vec{l}}\vec{u} \right)
%\end{equation}
%where $\vec{\lambda}$ solves equation~\eqref{eqn:generic_adjoint_system_design_opt} and the Jacobian $\frac{d\vec{u}}{d\vec{l}}$ is no more present. Furthermore the two-steps process of the reverse mode AD for the gradient computation is still there even in the adjoint method:
%\begin{itemize}
%	\item the forward pass, used for the computation of variables $v_i$ in table~\vref{tab:reverse_AD_example}, now is the single solution of the system $\vec{A}\vec{u}=\vec{b}$ in order to find the state $\vec{u}$ used in term~\eqref{eqn:resulting_term_adjoint_method};
%	\item the reverse pass, done in order to obtain the adjoint variables $\overline{v}_i$ in table~\ref{tab:reverse_AD_example}, is now equivalent to the solution of $\vec{A}^T \vec{\lambda} = \frac{\partial J}{\partial \vec{u}}$ (we would like to reiterate again that reverse and forward pass has the \emph{same} computational complexity). 
%\end{itemize}
%
%After this concise overview of the adjoint method applied to generic design optimization problems, we now proceed to the next section where we examine in more detail its application to those problems where the state $\vec{u}$ is obtained solving a Partial Differential Equation by means of RBF-FD methods.



\section{Design Optimization}

In everyday life the need for optimization arises almost naturally: birds optimize their wings' shape in real time, dogs optimize their trajectories in order to reach a specific place, and people constantly seek to improve their lives and the system that surround them. In these cases optimization is used as synonym of improvement, and from a mathematical standpoint it can be understood as the concept of: ``finding the best possible solution by changing variables that can be controlled, often subject to constraints''~\cite{Joaquim:engineering_design_opt}.

One simple way to solve general optimization problems would be a manual approach where in order to find the best possible solution of the problem a designer, the one in charge of solving the problem, has to adjust each single variable at time; however this approach has several limitation: it tends to lead suboptimal results and evaluating all possible variables might be too time-consuming. These issues become even more severe when tackling more complex problems like those found in engineering as: wing design in aerospace engineering, structural design in civil engineering, circuit design in electrical engineering and mechanism design in mechanical engineering.

Design optimization is the tool typically used to solve this last type of problems in order to accelerate the design cycle and obtain better results; it allows to automate the optimization by means of an optimizer, but in order to be used it requires a correct formulation of the problem that has to be solved, which includes:
\begin{itemize}
	\item the design variables that are to be changed;
	\item the objective that has to be minimized/maximized;
	\item the constraints that has to be met.
\end{itemize}
A more detailed explanation of the formulation, including the points mentioned above, will be provided in the next subsections.
%Then evaluation of a given design is performed by computing the numerical values of the objective and constraints.  % Da togliere (?)
% ---- OLD ----
%TODO: Cita che l'automazione viene raggiunta con un optimizer
%TODO: Aggiungi qualche spiegazione sui legami con il metodo dell'aggiunto
%TODO: Fai un' intro alle prossime subsection

% ---- NEW ----
%TODO: Chiudi con una spiegazipne delle prossime subsections

% OSS: Non dilungarti troppo su design variables, objective and constraints che dopo devi parlare anche dei "models and opt problems"

\subsection{Optimization problem formulation}

The mathematical formulation of an optimization problem is a $5$-steps procedure which is necessary in order to employ design optimization and it has the (positive) side effect of increasing the designer's insights about the problem. The steps are reported in figure~\ref{fig:opt_probl_math_formulation_steps} and we will explain each of this in this subsection.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{img/opt_probl_math_formulation_steps.pdf}
	\caption{Steps for the mathematical description of a optimization problem} 
	\label{fig:opt_probl_math_formulation_steps}
\end{figure}

\smallskip
At the very beginning the designer is required to write a description of the system and of the design problem, even if vague, with a list of all goals and requirements.

As a next step it has to gather as much information as possible about the problem in order to set its expectations at the right level (not too high, not too low) and to understand the constraints that the problem is subject to.
%In order to gain these insights about the optimization problem raw data might need to be processed and organized.
We also remark that:
\begin{itemize}
	\item in order to gain these insights about the optimization problem raw data might need to be processed and organized;
	\item information gathering and refinement are ongoing processes during the formulation of the problem and up to this point it is almost impossible for a designer to learn everything about the problem.
\end{itemize}
The next three steps, where mathematical definition for design variables, objective function and constraints comes into play, allows to further increase the clarity and the knowledge of the problem at hand.

\smallskip
Once the system's \emph{design variables} (also called \emph{design parameters}), which describe the system, are identified, can be represented by means of a vector:
\begin{equation}
	\vec{l} = [l_1, \dots, l_N]
\end{equation}
where it must be ensured that each design variable is independent of the others in order to allow the optimizer to perform a correct analysis of a specific design.
At each vector $\vec{l}$ is associated a single design and the number of variables $N$ defines the dimensionality of the problem that has to be solved. The design variables allow also to distinguish between two different classes of problems:
\begin{itemize}
	\item \emph{continuous optimization problems}, where variables are allowed to vary continuously within a range: $\underline{l}_i \le l_i \le \overline{l}_i$ for $i = 1, \dots, N$;
	\item \emph{discrete optimization problems}, in case variables are restricted to assume discrete set of values regardless of wether they are real or integer.
\end{itemize}

\smallskip
After choosing the design variables that identify a design, we need a quantity that tell us ``how good'' that design is. This quantity is obtained by means of the so called \emph{objective} or \emph{cost function} $J$, which is a scalar field that assign a number to a given design variable vector $\vec{l}$: with these numbers we can therefore define an order and compare different designs. Minimizing or maximizing the objective, depending on the problem at hand, allows to find the optimal design. What matters at this stage is the choice of a function that represent the real goal of the designer otherwise regardless of the precision with which the optimal design is identified, it will not be optimal from an engineering perspective.

\smallskip
As final step, the formulation of \emph{constraints} is required, these depend on the design variables and delineate the so called feasible region which is the set of allowed designs. These can be of two types:
\begin{itemize}
	\item \emph{inequality}, indicated with $g(\vec{l}) \le 0$. In literature is typically used the ``less or equal'' convention, but there is no loss of generality since a ``greater or equal'' constraint can be converted into ``less or equal'' simply by multiplying it by $-1$;
	\item \emph{equality}, indicated with $g(\vec{l}) = 0$. These constraints can also be seen as two separate constraints: $g(\vec{l}) \le 0$ and $g(\vec{l}) \ge 0$;
\end{itemize}
Even if at a first glance constraints may appear limiting and superfluous, they enable the optimizer to find designs that match designer's expectation.

\medskip
Now that we have gone through the $5$ steps described in this subsection a general design optimization problem can be defined as:
\begin{equation}
\label{eqn:general_design_opt_problem_no_model}
\begin{aligned}
	\text{minimize}   & \quad J(\vec{l})														   \\
	\text{by varying} & \quad \underline{l}_i \le l_i \le \overline{l}_i  & \quad i=1, \dots, N    \\
	\text{subject to} & \quad g_j(\vec{l})								  & \quad j=1, \dots, n_g  \\
					  & \quad h_k(\vec{l})								  & \quad j=1, \dots, n_h
\end{aligned}
\end{equation}
where we indicate with $n_g$ and $n_k$ the number of inequality and equality constraints respectively.
The minimum of $J$ is found automatically and iteratively by an optimizer starting from an initial design $\vec{l}_0$, as shown in figure~\ref{fig:analysis_phase_optimizer}. At each iteration the optimizer computes the values of the objective and constraint functions for the current design $\vec{l}$ (analysis phase) and then return $\vec{l}'$ which is an improvement of $\vec{l}$. The same process is then repeated using $\vec{l}'$ instead of $\vec{l}_0$ as input until $\vec{l}'$ coincides with $\vec{l}^*$ or it is deemed accurate enough, in the sense of $J$, by the designer.

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{img/analysis_phase_optimizer.pdf}
	\caption{Solution of an optimization problem. At the beginning the initial design $\vec{l}_0$ is provided to the optimizer, at each optimization step it produce a new design $\vec{l}$, from the previous one, and through the analysis it evaluates the objective $J$ and the constraints $g, h$ in order to produce a new design $\vec{l}'$. Once $\vec{l}'$ corresponds or is considered close enough to the optimum it is yield as the optimum design $\vec{l}^*$}
	\label{fig:analysis_phase_optimizer}
\end{figure}
%TODO: Parla di come viene effettuato l'aggiornamento del design aka gradient descend or ascend.

\subsection{Models and optimization problems}

In previous subsection we have introduced the concepts of objective and constraint functions, here we cover \emph{how} they are modeled and computed. These functions, as stated before, are evaluated in the analysis phase, and require the solution of a numerical model for a given design. A numerical model is obtained from the discretization of a mathematical model which in turn is a formal description of the physical system that undergoes the optimization.

\smallskip
%TODO: Specifica meglio la differenza tra u nel caso continuo e u nel caso vettoriale
Mathematical models are also referred as governing equations and determines the state of a physical system under specific conditions. Many of them consists of differential equations which require discretization in order to be solved, as an example, one can consider the one given in equation~\eqref{eqn:generic_continous_PDE}. After the discretization of such mathematical models we obtain a numerical model that can be written in residual form as:
\begin{equation}
	\label{eqn:numerical_model_residual_form}
	r_w(u_1, \dots, u_{n_r}) = 0, \qquad w=1, \dots, n_r
\end{equation}
where $u_1, \dots, u_{n_r}$ are the elements that compose the state vector $\vec{u}$.
%TODO: specifica meglio chi sono ste r e ste u
At this point the state $\vec{u}$ can be therefore found solving equation~\eqref{eqn:numerical_model_residual_form} by means of a proper solver.
%TODO: Aggiungi cenni ai vari tipi possibili di discretizzazione
%TODO: Aggiungi figura 3.3 o 3.20

Often the state of a model depends on a given design $\vec{l}$, which means that this dependency enter also in equations~\eqref{eqn:numerical_model_residual_form}. These can be rewritten as:
\begin{equation}
	r_w(\vec{u};\vec{l}), \qquad w=1, \dots, n_r
\end{equation}
where the semicolon is used to underline the fact that $\vec{l}$ is fixed when the equations are solved for $\vec{u}$.
This implies that the objective and the constraints, which depend on the system and thus on the state $\vec{u}$, in turn depend implicitly on $\vec{l}$: this means that they are fully determined by the design variables. By making this dependency explicit we can rewrite the general design optimization problem reported in equation~\eqref{eqn:general_design_opt_problem_no_model} as:
\begin{equation}
\label{eqn:general_design_opt_problem_with_model}
\begin{aligned}
	\text{minimize}   & \quad J(\vec{l}; \vec{u})														   \\
	\text{by varying} & \quad l_i  & \quad i=1, \dots, N    \\
	\text{subject to} & \quad g_j(\vec{l}; \vec{u})	 \le 0							  & \quad j=1, \dots, n_g  \\
	& \quad h_k(\vec{l}; \vec{u}) = 0								  & \quad k=1, \dots, n_h  \\
	& \quad \underline{l}_i \le l_i \le \overline{l}_i  & \quad i=1, \dots, N \\
	\text{while solving} & \quad r_w(\vec{u}; \vec{l}) = 0 & \quad w = 1, \dots, n_r  \\
	\text{by varying} & \quad u_w & \quad w = 1, \dots, n_r  \\
\end{aligned}
\end{equation}
where the last two lines are solved for $\vec{u}$ at each optimization step given the current design $\vec{l}$.
Therefore the overall structure of an optimization problem, reported in~\ref{fig:analysis_phase_optimizer}, can be adjusted by explicitly integrating the solver into the scheme, yielding the one reported in figure~\ref{fig:whole_opt_probl}.

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{img/whole_opt_probl.pdf}
	\caption{Optimization scheme, where the analysis phase, shaded in gray, is broken down into the solution of the numerical model obtained from the governing equations (in red) and the evaluation of objective and constraints (in blue)}
	\label{fig:whole_opt_probl}
\end{figure}
%TODO: Sfondo di Optimizer che dovrebbe essere bianco e non blu

From a different perspective the governing equations can be also considered as equality constraints which leads to the following different formulation:
\begin{equation}
\label{eqn:general_design_opt_problem_with_model_as_constraint}
\begin{aligned}
	\text{minimize}   & \quad J(\vec{l}, \vec{u})														   \\
	\text{by varying} & \quad l_i  & \quad i=1, \dots, N    \\
					  & \quad u_w & \quad w = 1, \dots, n_r  \\
	\text{subject to} & \quad g_j(\vec{l}; \vec{u})	\le 0							  & \quad j=1, \dots, n_g  \\
	& \quad h_k(\vec{l}; \vec{u}) = 0								  & \quad k=1, \dots, n_h  \\
	& \quad \underline{l}_i \le l_i \le \overline{l}_i  & \quad i=1, \dots, N \\
	& \quad r_w(\vec{u}; \vec{l}) = 0 & \quad w = 1, \dots, n_r  \\
\end{aligned}
\end{equation}
In this case design variables $u_1, \dots, u_{n_r}$ are considered as such since objective explicitly depends on them.
In practice, however, their values can vary only nominally since practically must satisfy constraints $ r_w(\vec{u}; \vec{l}) = 0$ for $ w = 1, \dots, n_r $. For this reason is possible to find problem formulations like the one reported above, but without the presence of the variables $u_1, \dots, u_{n_r}$ among those that can be varied. In any case formulations~\eqref{eqn:general_design_opt_problem_with_model} and~\eqref{eqn:general_design_opt_problem_with_model_as_constraint} are equivalent and leads to the same optimal design $\vec{l}^*$.


\subsection{Optimization Algorithms}

So far we know that the optimizer. at each optimization step, update the current design producing a new one. In this subsection we delve in more detail how this update, performed by means of an optimization algorithm, can be produced.
%TODO: più che altro su quelli che andiamo ad usare in questa tesi...

A multitude of algorithms can be used to determine the aforementioned update and can be divided into diverse families, two of the most important being:
\begin{description}
	\item[Mathematical versus Heristic] For their proper functioning two of the essentials they need are: iterative process and evaluation criteria. The former is required in order to determine the sequence of points in the design space evaluated during the optimization, the latter in order to stop the search.
	
	Heuristics, unlike the mathematical algorithm based on mathematical principles, are based on rule of thumb for the iterative process and the evaluation criterion. Algorithms that mix mathematical arguments with heuristic can also be defined.
	
	
	\item[Information order] As can be also seen from the scheme reported in figure~\ref{fig:whole_opt_probl}, the optimizer require the values of the objective and constraints for a fixed design: these are zeroth-order information. If the above-mentioned are the only values required by the algorithm to perform the update, then it takes the name of zeroth-order or or gradient-free algorithm.
	
	We instead call first-order or gradient-based those algorithms which make use of gradients of one or both of the objective and constraint functions with respect to design variables (which are fist-order informations). Their main advantages respect the zeroth-order algorithms are: better scalabity of function evaluations with the number of design variables~\cite{Joaquim:engineering_design_opt} and the possibility to easily check if a point in the design space satisfies optimality condition.
	
	Algorithms that make use of second-order information also exists: an example are those that compute the curvature of the objective which indicate an idea of where the function might flatten out, giving more insight into the function behavior beyond simple steepness.
	
	A final clarification about the information order is about the difference between the one provided by the user and the one actually used by the algorithm, which might differ. An example of this difference is the finate difference algorithm which estimate a first-order quantity, the gradient, starting from the function values provided by the user. In this case the gradient estimate requires additional function evaluations compared to the case where user provides the gradient value themselves.
\end{description}

For the rest of this work we will consider only the following optimization algorithm:
\begin{equation}
	\label{eqn:our_update_rule_for_opt_problem}
	\vec{l}_{k+1} = \vec{l}_k + \alpha \frac{d J}{d \vec{l}}
\end{equation}
%TODO: Domanda ci vanno partial o total derivatives
where the optimizer, at each $k$-th steps, yields the improved design identified by $\vec{l}_{k+1}$ by increasing the value of each design variable of the current design $\vec{l}_k$ of a quantity proportional to the gradient of the objective respect to the design variables. This proportionality is controlled by the parameter $\alpha$, which is fixed at the beginning of the optimization and remains constant throughout the process.

Concerning the classification presented in this subsection we can frame~\eqref{eqn:our_update_rule_for_opt_problem} as an optimization algorithm that is based on a mathematical rationale with a mix of heuristics introduced by the choice of the parameter $\alpha$. It is also a first-order algorithm since the update require explicitly the gradient of the objective.
The primary drivers behind the choice of this optimizer were its mathematical foundations, its simple implementation and its effectiveness with with linear and quadratic objectives. Of course others approaches exists and may be more or less effective depending on the type of the problem that has to be faced. Some examples are:
\begin{description}
	\item[Brute-force] In this case all possible combinations of design parameters are evaluated. This approach could be useful in case of very simple design optimization problems with few design variables, but results to be time-expensive for large-scale projects where parameters can be hundreds, thousands, or even more and the optimal design may not found in reasonable time-frames.
	\item[Genetic Algorithms] These are particularly useful when objective and constraint functions are not sufficiently smooth or derivatives can not be computed with enough precision. A key trait of these algorithms is that they require many iterations for convergence, which is sensitive to the cost of the function evaluations, and scales poorly with the number of design parameters~\cite{Joaquim:engineering_design_opt}.
%	TODO: aggiungi cit ad Esteco
\end{description}

%These functions are tipically related to some physical model, so they are basically obtained by modeling a the particular phisical system in a mathematical model
%In design optimization, in particular during the analysis phase, we need to evaluate both

\subsection{Design optimization with RBF-FD models}

One of the main areas where the application of RBF-FD method bring great benefits is Computational Fluid Dynamics (CFD) where PDEs are employed to model fluid flow and heat transfer problems. In CFD, in general, many design optimization problems arise. A typical case is wing design where an example of the results that can be achieved is presented in figure~\ref{fig:wing_opt_new}.
%TODO: Tra questo e il paragrafo successivo controlla che ci sono def. di anagrammi messi un po' a caso prima e dopo

\begin{figure}
	\centering
	\includegraphics[width=.5\textwidth]{img/wing_opt.png}
	\caption{Example of design optimization: baseline and final design of ONERA M6 wing is shown. It can be seen how the relative pressure exerted by the air on the wing has been minimized. Figure taken from~\cite{Bombardieri:paper_of_wing_opt_image}}
	\label{fig:wing_opt_new}
\end{figure}

In section~\vref{sec:RBF-FD} we have seen how the the RBF-FD solver for Partial Differential Equations (PDEs), which can be employed to solve CFD related equations as well, is implemented: starting from a point cloud distributed over the physical domain, it yields a set of equations which once solved allows to find the values of the approximating field on the aforementioned nodes.
In this scenario a PDE is, in every respect, what we have defined as mathematical model or governing equations when discussing optimization problem in previous section. Moreover, for these cases, is easy to identify the numerical model as the one obtained by the RBF-FD method since it actually discretize the PDE.

Keeping in mind what is stated above a typical optimization problem which depends on the solution, found by means of an RBF-FD solver, of a model based on a PDE, can be stated as: 
\begin{equation}
	\label{eqn:general_design_opt_problem_with_RBF-FD_model}
	\begin{aligned}
		\text{minimize}   & \quad J(\vec{l}, \vec{u})														   \\
		\text{by varying} & \quad l_i  & \quad i=1, \dots, N    \\
		\text{subject to} & \quad g_j(\vec{l}; \vec{u})	\le 0							  & \quad j=1, \dots, n_g  \\
		& \quad h_k(\vec{l}; \vec{u}) = 0								  & \quad k=1, \dots, n_h  \\
		& \quad \underline{l}_i \le l_i \le \overline{l}_i  & \quad i=1, \dots, N \\
		& \quad \vec{C}_I\vec{u}_I + \vec{C}_B\vec{u}_B - \vec{f} = \vec{0}  \\
	\end{aligned}
\end{equation}
where in the last constraint has been used the same notation presented in chapter~\ref{chap:RBF-FD_method}.
%TODO: Fixa il chapter 1 che ha reference sbagliata
For its solution it can be implemented the scheme reported in figure~\ref{fig:whole_opt_probl}.

%TODO: Il gradient deve essere calcolato e come lo calcoli?? ci dedichi i 2 capitoli successivi
In the particular case when the optimizer block update the design following the rule reported in~\eqref{eqn:our_update_rule_for_opt_problem} the derivative of the objective $J$ with respect to the design variables needs to be computed. In general this can be computed through the application of the chain rule as:
\begin{equation}
	\label{eqn:generic_gradient_design_opt}
	\frac{dJ}{d\vec{l}}^T = \frac{\partial J}{\partial \vec{u}}^T \frac{d\vec{u}}{d\vec{l}} + \frac{\partial J}{\partial \vec{l}}^T
\end{equation}
where the notations $d/dx$ and $\partial/\partial x$ indicate the total and the partial derivatives respect the variable $x$. And if terms $\partial J / \partial \vec{u}$ and $\partial J / \partial \vec{l}$ can be efficiently obtained by means of reverse-mode AD, as explained in section~\vref{subsec:reverse_mode_AD}, since $J$ is known analytically, matrix $\partial \vec{u} / \partial \vec{l}$, require more careful considerations, since the differentiation of the constrains $\vec{C}_I\vec{u}_I + \vec{C}_B\vec{u}_B - \vec{f} = \vec{0}$ is required for its computation. In the following section we explain the details for the computations of $dJ/d\vec{l}$ using the adjoint method, which simplify the optimization of problems with RBF-FD-like constraints.



\section{Adjoint method in RBF-FD based Design Optimization}
\label{sec:adjoint_method}

In previous section we had a general overview on design optimization problems. In this section we will see how those problems based on RBF-FD models can be solved using the adjoint method in the optimization step. In particular we focus on two specific types of problems respectively in one and three-dimensional spaces. Before that we will explain more generally how the adjoint method can be employed in the computation of $dJ / d\vec{l}$, used for the update of the design variables at each step, and which benefits it brings.


\subsection{Adjoint intro}

We start by considering a generic design optimization problem with the following formulation:
\begin{equation}
	\label{eqn:super_general_design_opt_problem_with_RBF-FD_model}
	\begin{aligned}
		\text{minimize}   & \quad J(\vec{l}, \vec{u})			\\
		\text{by varying} & \quad l_i  & \quad i=1, \dots, N    \\
		\text{subject to} & \quad \vec{A} \vec{u} = \vec{b}
	\end{aligned}
\end{equation}
where the optimization algorithm used for its solution is the one that employs equation~\eqref{eqn:our_update_rule_for_opt_problem}, which require the computation of the gradient of the objective, to update the design parameters at each step.

To directly evaluate $dJ / d\vec{l}$, in case of a particular set of design parameters defined by $\vec{l}$ at a given state $\vec{u}$, following equation~\eqref{eqn:generic_gradient_design_opt}, we would compute:
\begin{equation}
	\label{eqn:generic_gradient_design_opt_adjoint_chapter}
	\frac{dJ}{d\vec{l}}^T = \frac{\partial J}{\partial \vec{u}}^T \frac{d\vec{u}}{d\vec{l}} + \frac{\partial J}{\partial \vec{l}}^T
\end{equation}
%TODO: Valuta di precisare la notazione da qualce parte invece di metterla in mezzo al documento
$\partial J / \partial \vec{u}$ and $\partial J / \partial \vec{l}$ can be efficiently obtained by means of reverse-mode AD, as explained in the previous subsection. The matrix $d\vec{u} / d\vec{l}$, on the other hand, requires more careful considerations which we address here.

\smallskip
Differentiating the problem constraints $\vec{Au} = \vec{b}$ with respect to the parameters $\vec{l}$ gives:
\begin{equation}
	\frac{d\vec{A}}{d\vec{l}} \vec{u} + \vec{A} \frac{d\vec{u}}{d\vec{l}} = \frac{d\vec{b}}{d\vec{l}}
\end{equation}
which allows to obtain the sought term $d\vec{u} / d\vec{l}$ as:
\begin{equation}
	\label{eqn:state_sensitivity_respect_single_param}
	\frac{d\vec{u}}{d\vec{l}} = \vec{A}^{-1} \left( \frac{d\vec{b}}{d\vec{l}} - \frac{d\vec{A}}{d\vec{l}}\vec{u} \right)
\end{equation}
We notice that $d\vec{u} / d\vec{l}$ is a matrix of size $M \times N$ whose $j$-th column represent the sensitivity of the state $\vec{u}$ respect to the parameter $l_j$. 
In practice it is populated column-wise by solving equation~\eqref{eqn:state_sensitivity_respect_single_param} $N$ times when the derivative is taken with respect the $j$-th design parameter $l_j$ rather than $\vec{l}$: this gives its $j$-th column. In this process $d\vec{b} / d\vec{l}$ and $d\vec{A} / d\vec{l}$ terms can be simply obtained using AD, if $\vec{b}$ and $\vec{A}$ are known analytically, and once $d\vec{u} / d\vec{l}$ is computed it can be substituted in equation~\eqref{eqn:generic_gradient_design_opt} to obtain the gradient which symbolically reads as:
\begin{equation}
	\label{eqn:generic_gradient_design_opt_naive}
	\frac{dJ}{d\vec{l}} = \frac{\partial J}{\partial \vec{u}}^T \left[ \vec{A}^{-1} \left( \frac{d\vec{b}}{d\vec{l}} - \frac{d\vec{A}}{d\vec{l}}\vec{u} \right) \right]  + \frac{\partial J}{\partial \vec{l}}^T
\end{equation}

However the computational costs of this naive procedure scales linearly with the number of the design parameters $N$, since $N$ solutions of linear systems are required in order to construct $d\vec{u} / d\vec{l}$. We finally remark that each of these inversions has the same computational cost of solving $\vec{A} \vec{u}= \vec{b}$ for $\vec{u}$. In a scenario where CFD simulations are involved this is particularly penalizing, therefore a more efficient gradient computation is required.

\smallskip
To do so we can employ the \emph{adjoint method} which simply consist of a smarter bracketing of the equation which gives the gradient. In fact is possible to rewrite~\eqref{eqn:generic_gradient_design_opt_naive} as:
\begin{equation}
	\begin{aligned}
		\frac{dJ}{d\vec{l}} & = \left[ \frac{\partial J}{\partial \vec{u}}^T \vec{A}^{-1} \right] \left( \frac{d\vec{b}}{d\vec{l}} - \frac{d\vec{A}}{d\vec{l}}\vec{u} \right) + \frac{\partial J}{\partial \vec{l}}^T  \\[2ex]
		& = \vec{\lambda}^T \left( \frac{d\vec{b}}{d\vec{l}} - \frac{d\vec{A}}{d\vec{l}}\vec{u} \right) + \frac{\partial J}{\partial \vec{l}}^T
	\end{aligned}
\end{equation}
where the vector $\vec{\lambda} \in \R^L$, whose elements are called \emph{adjoint variables}, can be found by solving:
\begin{equation}
	\label{eqn:generic_adjoint_system_design_opt}
	\vec{A}^T \vec{\lambda} = \frac{\partial J}{\partial \vec{u}}
\end{equation}

By doing so is possible to solve the system in equation~\eqref{eqn:generic_adjoint_system_design_opt}, which is called \emph{adjoint problem}, only \emph{once} independently on the number $N$ of design parameters for each gradient computation: this significantly reduces the computational burden of the whole optimization process. Bear in mind that the adjoint problem has the same size as the problem defined by the constraints $\vec{A} \vec{u} = \vec{b}$ and the computational cost for their solution is the same.

\bigskip
%At this point it is worth noting that the adjoint method explained above is equivalent to reverse mode AD. To make the connections between the two methods clearer we note that the first computation of the  objective function's gradient, presented in equation~\eqref{eqn:generic_gradient_design_opt}, contains the following vector-Jacobian product:
%\begin{equation}
%	\label{eqn:vJp_design_opt}
%	\frac{\partial J}{\partial \vec{u}}^T \frac{d\vec{u}}{d\vec{l}}
%\end{equation}
%where $(\partial J / \partial \vec{u})^T \in \R^M$ is the vector that multiply $(d\vec{u} / d\vec{l}) \in \R^{M \times N}$, the Jacobian of the state $\vec{u}$ with respect to the design parameters $\vec{l}$.
%%TODO. The next phrase which does not have an end has been written to connect adjoint to AD.
%%What the adjoint method does is, instead of computing directly $\frac{dJ}{d\vec{l}}$, first solves .
%From subsection~\ref{subsec:reverse_mode_AD} we known that AD allows to compute the resulting vector without explicitly computing the Jacobian $\frac{d\vec{u}}{d\vec{l}}$. But this is precisely what the adjoint method does through equations~(\ref{eqn:generic_gradient_design_opt} -~\ref{eqn:generic_adjoint_system_design_opt}) by first applying the chain rule to the leftmost term in the gradient formula and then parenthesizing the result in order to make the number of inversions of the matrix $\vec{A}$ independent on the number of parameters.
The result of the whole process is then summarized by:
\begin{equation}
	\label{eqn:resulting_term_adjoint_method}
	\vec{\lambda}^T \left( \frac{d\vec{b}}{d\vec{l}} - \frac{d\vec{A}}{d\vec{l}}\vec{u} \right)
\end{equation}
where $\vec{\lambda}$ solves equation~\eqref{eqn:generic_adjoint_system_design_opt} and the Jacobian $d\vec{u} / d\vec{l}$ is no more present. From the process outlined here, we can observe some commonalities between the adjoint method and reverse mode automatic differentiation (AD):
\begin{itemize}
	\item both apply the chain rule to decompose the computation of the gradient;
	\item both propagate derivatives in reverse: reverse mode AD propagates from a single output of a function back to its inputs, the adjoint does the same from the objective back to the design variables;
	\item both are efficient in case of large number of inputs.
\end{itemize}
Furthermore adjoint method also share a two-steps process with the reverse mode AD:
\begin{itemize}
	\item the forward pass, used for the computation of variables $v_i$ in table~\vref{tab:reverse_AD_example}, now is the single solution of the system $\vec{A}\vec{u}=\vec{b}$ in order to find the state $\vec{u}$ used in term~\eqref{eqn:resulting_term_adjoint_method};
	\item the reverse pass, done in order to obtain the adjoint variables $\overline{v}_i$ in table~\ref{tab:reverse_AD_example}, is now the solution of $\vec{A}^T \vec{\lambda} = \frac{\partial J}{\partial \vec{u}}$ (we would like to reiterate again that reverse and forward pass has the \emph{same} computational complexity). 
\end{itemize}

After this concise overview of the adjoint method applied to generic design optimization problems, we now proceed to the next section where we examine in more detail its application to those problems where the state $\vec{u}$ is obtained solving a Partial Differential Equation by means of RBF-FD methods.

\subsection{1D case}
\label{subsec:adjoint_method_RBF-FD_1D}

In this case we consider a generic cost function $J \colon \R^{N_I \times L} \to \R$ which depends on the $N_I$ values of the field $u$ at the points located within the physical domain and on the $L$ design parameters. The resulting optimization problem is:
\begin{equation}
	\label{eqn:design_opt_problem_with_RBF-FD_model_1D}
	\begin{aligned}
		\text{minimize}   & \quad J(\vec{l}, \vec{u})														   \\
		\text{by varying} & \quad l_i  & \quad i=1, \dots, N    \\
		\text{subject to} & \quad \vec{C}_I\vec{u}_I + \vec{C}_B\vec{u}_B - \vec{f} = \vec{0}  \\
	\end{aligned}
\end{equation}
Thanks to this generality its sensitivities with respect to the parameters, are given by the same exact formula reported in~\eqref{eqn:generic_gradient_design_opt}:
\begin{equation}
	\label{eqn:gradient_1st_step_1D_RBF-FD}
	\frac{dJ}{d\vec{l}}^T = \frac{\partial J}{\partial \vec{u}_I}^T \frac{d\vec{u}_I}{d\vec{l}} + \frac{\partial J}{\partial \vec{l}}^T
\end{equation}
On the above formula the term that needs to be computed carefully is the matrix $d\vec{u}_I / d\vec{l}$. The sensitivities of the state with respect to the parameters are found by differentiating equation~\eqref{eqn:compact_discretized_PDE} which gives:
\begin{equation}
	\label{eqn:1D_differentiated_constraint}
	\frac{d\vec{C}_I}{d\vec{l}} \vec{u}_I + \vec{C}_I \frac{d\vec{u}_I}{d\vec{l}} + \frac{d\vec{C}_B}{d\vec{l}} \vec{u}_B + \vec{C}_B \frac{d\vec{u}_B}{d\vec{l}} -
	\frac{d\vec{f}}{d\vec{l}} = \vec{0}
\end{equation}
Before proceeding a note on the matrix $\frac{d\vec{u}_B}{d\vec{l}}$ could be useful: one might think that it should not be present since values in $\vec{u}_B$ are fixed from boundary conditions of problem~\eqref{eqn:boundary_value_problem} (i.e. they are not dependent on the design parameters).
Nevertheless the previous statement does not hold in general since the values of $\vec{u}_B$ could directly depend on the design variables (as an example one may think of Dirichlet Boundary Conditions (BCs) defined by $u(\vec{x})=g(\vec{x}, \vec{l})$) or indirectly depend on them through the problem geometry (e.g. Neumann or Robin BCs). Rearranging~\eqref{eqn:1D_differentiated_constraint} we obtain the sought term:
\begin{equation}
	\frac{d \vec{u}_I}{d \vec{l}} = \vec{C}_I^{-1} \biggl( \frac{d\vec{f}}{d\vec{l}} - \frac{d\vec{C}_I}{d\vec{l}} \vec{u}_I - \frac{d\vec{C}_B}{d\vec{l}} \vec{u}_B - \vec{C}_B \frac{d\vec{u}_B}{d\vec{l}} \biggr)
\end{equation} 
Then plugging the found expression for $d\vec{u}_I / d\vec{l}$ in~\eqref{eqn:gradient_1st_step_1D_RBF-FD} yields:
\begin{equation}
	\begin{aligned}
		\frac{dJ}{d\vec{l}}^T & = \frac{\partial J}{\partial \vec{u}_I}^T \biggl[ \vec{C}_I^{-1} \biggl( \frac{d\vec{f}}{d\vec{l}} - \frac{d\vec{C}_I}{d\vec{l}} \vec{u}_I - \frac{d\vec{C}_B}{d\vec{l}} \vec{u}_B - \vec{C}_B \frac{d\vec{u}_B}{d\vec{l}} \biggr) \biggr] + \frac{\partial J}{\partial \vec{l}}^T  \\[2ex]
		& = \frac{\partial J}{\partial \vec{u}_I}^T \biggl[ \vec{C}_I^{-1} \biggl( \frac{d\vec{f}}{d\vec{l}} - \frac{d\vec{C}}{d\vec{l}} \vec{u} - \vec{C}_B \frac{d\vec{u}_B}{d\vec{l}} \biggr) \biggr] + \frac{\partial J}{\partial \vec{l}}^T
	\end{aligned}
\end{equation}
where $d\vec{C} / d\vec{l} \in \R^{N_I \times N}$ is the matrix resulting from the concatenation of the rows of $d\vec{C}_I / d\vec{l} \in \R^{N_I \times N_I}$ and $d\vec{C}_B / d\vec{l} \in \R^{N_B \times N_B}$, and $\vec{u} \in \R^{N}$ is the vector given by the concatenation of the elements of $\vec{u}_I \in \R^{N_I}$ and $\vec{u}_B \in \R^{N_B}$.
Now the adjoint method can be employed in order to avoid performing the inversion of matrix $\vec{C}_I$ once for each parameter in $\vec{l}$, yielding:
\begin{equation}
	\frac{dJ}{d\vec{l}}^T =  \vec{\lambda}_1^T \biggl( \frac{d\vec{f}}{d\vec{l}} - \frac{d\vec{C}}{d\vec{l}} \vec{u} - \vec{C}_B \frac{d\vec{u}_B}{d\vec{l}} \biggr) + \frac{\partial J}{\partial \vec{l}}^T  \\
\end{equation}
where $\vec{\lambda}_1$ is found by solving \emph{once}:
\begin{equation}
	\vec{C}_I^T \vec{\lambda}_1 = \frac{\partial J}{\partial \vec{u}_I}
\end{equation}
The unknown terms involving derivatives computation on the right-hand side can be computed easily through AD except for $\frac{d\vec{C}}{d\vec{l}} \vec{u}$. This terms deserves more attentions since requires the differentiation of the global matrix $\vec{C}$ which is obtained by solving $N_I$ local systems, each associated with a different stencil $\mathcal{X}_i$.

\medskip
%Taking a moment to consider the matrix $\frac{d\vec{C}}{d\vec{l}} \vec{u} \in \R^{N \times L}$ we can note that is obtained from a product between a rank-3 tensor 
In order to examine it more closely we can define a matrix $\vec{Q} \in \R^{N_I \times L}$ as follows:
\begin{equation}
	\label{eqn:Q_matrix_definition}
	\vec{Q} = \frac{d\vec{C}}{d\vec{l}} \vec{u}
\end{equation}
whose elements $q_{i,j}$ are given by:
\begin{equation}
	\label{eqn:Q_elements}
	q_{i,j} = \frac{d\vec{C}_{[i,:]}}{dl_j} \vec{u}
\end{equation}
where notation $\vec{C}_{[i,:]}$ is used to indicate the $i$-th row of the matrix $\vec{C}$.
To obtain $\frac{d\vec{C}}{d\vec{l}} \vec{u}$ is thus sufficient to compute each element of $\vec{Q}$ through equation~\eqref{eqn:Q_elements}. However, before doing so, it is worth rewriting the equation for $q_{i,j}$ more explicitly.

Recall that elements that make up the $i$-th row of $\vec{C}$ are found by solving the local system~\eqref{eqn:row_of_C_system}:
\begin{equation}
	\label{eqn:local_RBF-FD_system_adjoint_1D}
	\vec{M}_{BC}^T
	\begin{bmatrix}
		\vec{c}_I(\vec{x}_i)  \\
		\vec{c}_B(\vec{x}_i)  \\
		\vec{c}_p(\vec{x}_i)
	\end{bmatrix} = 
	\begin{bmatrix}
		\mathcal{L} \vec{\Phi}(\vec{x}_i, \mathcal{X}_{i,I})  \\
		\mathcal{L} \vec{p}(\vec{x}_i)
	\end{bmatrix}
\end{equation}
in case of RBF-FD method, or the local system~\eqref{eqn:row_of_C_system_in_RBF-HFD}:
\begin{equation}
	\label{eqn:adjoint_method_row_of_C_system_in_RBF-HFD}
	\vec{M}_{BC}^T
	\begin{bmatrix}
		\vec{c}_I(\vec{x}_i)  \\
		\vec{c}_B(\vec{x}_i)  \\
		\vec{c}_p(\vec{x}_i)
	\end{bmatrix} = 
	\begin{bmatrix}
		\mathcal{L}_1 \vec{\Phi}(\vec{x}_i, \mathcal{X}_{i,I})  			  \\
		\mathcal{L}_1 \mathcal{B}_2 \vec{\Phi}(\vec{x}_i, \mathcal{X}_{i,B})  \\
		\mathcal{L} \vec{p}(\vec{x}_i)
	\end{bmatrix}
\end{equation}
in case of RBF-HFD method. The key aspect that both cases share is that the aforementioned equations arise from the information contained in a single stencil $\mathcal{X}_i$ and, as explained in subsection~\vref{subsec:RBF-FD_formulation}, only the first $m$ elements of $\vec{c}(\vec{x}_i) = \bigl[ \vec{c}_I(\vec{x}_i), \vec{c}_B(\vec{x}_i), \vec{c}_p(\vec{x}_i) \bigr]$ form the $i$-th row of $\vec{C}$. 
Furthermore, since $m \ll N$, row $\vec{C}_{[i,:]}$ is sparse; which in turns implies that not all the elements of $\vec{u}$ are involved in the computation of $q_{i,j}$.

By letting $\vec{\tilde{u}}_i \in \R^m$ the vector composed by the components of $\vec{u}$ associated to the non zero elements of $\vec{C}_{[i,:]}$, we can rewrite the elements of $\vec{Q}$ reported in~\eqref{eqn:Q_elements} as:
\begin{equation}
	\label{eqn:Q_elements_rewritten}
	q_{i,j} =
	\frac{d\vec{c}(\vec{x}_i)^T}{dl_j}
	\begin{bmatrix}
		\vec{\tilde{u}}_i  \\
		\vec{0}
	\end{bmatrix}
\end{equation}
where the zero-vector concatenated to $\vec{\tilde{u}}_i$ has the same length of $\vec{c}_p(\vec{x}_i)$.
If we now differentiate equation~\eqref{eqn:adjoint_method_row_of_C_system_in_RBF-HFD} with respect to the $j$-th design parameter, we obtain:
\begin{equation}
	\frac{d \vec{M}_{BC}^T}{d l_j} \vec{c}(\vec{x}_i) + \vec{M}_{BC}^T \frac{d \vec{c}(\vec{x}_i)}{d l_j} = \frac{d \vec{h}}{d l_j}
\end{equation}
where the vector $\vec{h} \in \R^{m+M}$ is used as a shorthand for the vector present on the right-hand side of equation~\eqref{eqn:adjoint_method_row_of_C_system_in_RBF-HFD}. Now from the last equation we can isolate:
\begin{equation}
	\frac{d \vec{c}(\vec{x}_i)^T}{dl_j} = \left( \frac{d \vec{h}}{d l_j} - \frac{d \vec{M}_{BC}^T}{d l_j} \vec{c}(\vec{x}_i) \right)^T \vec{M}_{BC}^{-1}
\end{equation}
which once plugged in~\eqref{eqn:Q_elements_rewritten} allows to rewrite $q_{i,j}$ as:
\begin{equation}
	\label{eqn:Q_elems_before_adjoint}
	q_{i,j} =
	\left[ \left( \frac{d \vec{h}}{d l_j} - \frac{d \vec{M}_{BC}^T}{d l_j} \vec{c}(\vec{x}_i) \right)^T \vec{M}_{BC}^{-1} \right]
	\begin{bmatrix}
		\vec{\tilde{u}}_i  \\
		\vec{0}
	\end{bmatrix}
\end{equation}
But now we can clearly see that in order to populate the matrix $\vec{Q}$ we should invert $N_I \times L$ matrices $\vec{M}_{BC}$ of size $(m+M) \times (m+M)$ since we need to compute $\frac{d \vec{c}(\vec{x}_i)^T}{dl_j}$. These matrices are smaller than $\vec{C}_I$, but in any case their inversion is still problematic, since both the number of parameters and the number of nodes are huge. The problem, here, is very similar to the one faced in the previous subsection where there was a linear relationship between the number of matrix inversions and the number of design variables: consequently, even here, to improve the situation it is possible to rely on the adjoint method. Modifying the parentheses of equation~\eqref{eqn:Q_elems_before_adjoint} it is possible to write:
\begin{equation}
	\begin{aligned}
		q_{i,j} & =
		\left( \frac{d \vec{h}}{d l_j} - \frac{d \vec{M}_{BC}^T}{d l_j} \vec{c}(\vec{x}_i) \right)^T
		\left(
		\vec{M}_{BC}^{-1}
		\begin{bmatrix}
			\vec{\tilde{u}}_i  \\
			\vec{0}
		\end{bmatrix}
		\right)  \\
		& =
		\left( \frac{d \vec{h}}{d l_j} - \frac{d \vec{M}_{BC}^T}{d l_j} \vec{c}(\vec{x}_i) \right)^T
		\vec{\lambda}_{2,i}
	\end{aligned}
\end{equation}
where the adjoint vector $\vec{\lambda}_{2,i} \in \R^{m+M}$ is found as solution of:
\begin{equation}
	\label{eqn:second_adjoint_in_RBF-HFD}
	\vec{M}_{BC} \vec{\lambda}_{2,i} =
	\begin{bmatrix}
		\vec{\tilde{u}}_i  \\
		\vec{0}
	\end{bmatrix}
\end{equation}
and has to be computed once for each row of $\vec{Q}$ (or equivalently $\frac{d\vec{C}}{d\vec{l}} \vec{u}$). The crucial point is that this second adjoint implementation allows to make the computational cost to obtain $\frac{d\vec{C}}{d\vec{l}} \vec{u}$ independent of the number of design parameters $L$: the number of systems~\eqref{eqn:second_adjoint_in_RBF-HFD} that has to be solved, whose cost is similar to the inversion of $\vec{M}_{BC}$, is now always $N_I$.

\smallskip
In light of what we have observed we are now able to compute the whole gradient $\frac{dJ}{d\vec{l}}^T$ with a computation effort analogous to two RBF-HFD:
\begin{enumerate}
	\item a first plain application of RBF-HFD is required in order to compute the terms $\vec{u}$, $\vec{C}_I$ and $\vec{C}_B$;
	\item then, a comparable cost is required for the application of the presented adjoint method as shown in table~\ref{tab:RBF-FD_and_adjoint_comparison}.
\end{enumerate}

\begin{table}
	\caption{Systems that demands the computation of a solution during the application of RBF-FD and the adjoint method explained in section~\ref{subsec:adjoint_method_RBF-FD_1D}. Those of dimension $m+M$ are solved once for each stencil in the physical domain}
	\label{tab:RBF-FD_and_adjoint_comparison}
	\centering
	\begin{tabular}{ccc}
		\toprule
		Method										&  Solved systems															& System dimension				\\
		\midrule
		\multirow{2}*{RBF-FD}						& $\vec{C}_I \vec{u}_I = f - \vec{C}_B \vec{u}_B$ 							& $N_I$							\\[1.5ex]
		& $\vec{M_{BC}}^T \vec{c}(\vec{x}_i) = \vec{h}$ for $i=1,\dots,N_I$								& $m+M$							\\[2ex]
		\multirow{2}*{Adjoint}						& $\vec{C}_I^T \vec{\lambda}_1 = \frac{\partial J}{\partial \vec{u}_I}$		& $N_I$							\\[1.5ex]
		& $\vec{M_{BC}} \vec{\lambda}_{2,i} =
		\begin{bmatrix}
			\vec{\tilde{u}}_i  \\
			\vec{0}
		\end{bmatrix}$ for $i=1,\dots,N_I$											& $m+M$						\\[1.5ex]
		\bottomrule
	\end{tabular}
\end{table}


%thus yields only $m$ elements, where $m$ is the number of nodes within the stencil. This means that $\vec{C}_{[i,:]}$ row is composed by just $m \ll N$ non-zero elements and thus not all the components of $\vec{u}$ are involved in the computation of $q_{i,j}$.
%By letting $\vec{\tilde{c}}_i$ represent the vector formed by the non-zero elements of $\vec{C}_{[i,:]}$ padded with a number of zero elements in order to and $\vec{\tilde{u}}_i$ the vector composed by the elements of $\vec{u}$ associated to $\vec{\tilde{c}}_i$ we can rewrite the elements of $\vec{Q}$ reported in~\eqref{eqn:Q_elements} as:
%\begin{equation}
%	q_{i,j} = \frac{d\vec{\tilde{c}}_i}{dl_j}
%\end{equation}
%Leveraging on this newly introduced notation we can also rewrite equations~\eqref{eqn:row_of_C_system} and~\eqref{eqn:row_of_C_system_in_RBF-HFD}. The second one, related to RBF-HFD method, becomes:
%\begin{equation}
%	\label{eqn:row_of_C_system_new_notation}
%	\vec{M_{BC}}^T
%	\underbrace{
	%	\begin{bmatrix}
		%		\vec{\tilde{c}}_i		\\
		%		\vec{c}_P(\vec{x}_i)
		%	\end{bmatrix}
	%	}_{\vec{c}(\vec{x}_i)}
%	=
%	\begin{bmatrix}
	%		\mathcal{L}_1 \vec{\Phi}(\vec{x}_i, \mathcal{X}_{i,I})  			  \\
	%		\mathcal{L}_1 \mathcal{B}_2 \vec{\Phi}(\vec{x}_i, \mathcal{X}_{i,B})  \\
	%		\mathcal{L} \vec{p}(\vec{x}_i)
	%	\end{bmatrix}
%\end{equation}
%If we now differentiate~\eqref{eqn:row_of_C_system_new_notation} respect the $j$-th design parameter we get:
%%\begin{equation}
%%	\frac{\partial \vec{M_{BC}}^T}{\partial l_j} \vec
%%\end{equation}




%TODO: aggiungi un po' di dimensioni dei vettori in giro

\subsection{3D case}
\label{subsec:adjoint_method_RBF-FD_3D}

Now, unlike in the 1D case, we consider only cost functions which describes a flux integral, i.e. functions that can be reduced into a form similar to:
\begin{equation}
	\label{eqn:continuous_3D_cost_function}
	J_c = \frac{1}{\abs{A_c}} \int_A \frac{\partial u}{\partial \vec{n}} \, ds
\end{equation}
where $\abs{A_c} \in \R$ indicates the area of the surface $A$ over which the integral is calculated, $\vec{n}$ is the normal of the surface and $u$ is a scalar field.

Since in our case the physical domain is described by means of \verb*|.stl| files, we will not deal with continuous quantities and the integral will therefore be approximated by the following finite sum:
\begin{equation}
	J = \frac{1}{\abs{A}} \sum_{j \in \mathcal{T}} \nabla \vec{u}(\vec{x}_j)^T \vec{a}_j
\end{equation}
where $\abs{A} \in \R$ is still the area of the surface $A$, this time computed as sum of the areas of each small triangle that composes it, $\mathcal{T}$ is the set of the indices of these triangles and each of them is identified by a single index $j$; $\vec{x}_j \in \R^3$ and $\vec{a}_j \in \R^3$ denotes respectively the centroid and the area vector of the $j$-th triangle, where $\vec{a}_j$ has direction given by the normal and modulus equal to the surface of the $j$-th triangle.
$\nabla \vec{u}(\vec{x}_j)^T \in \R^3$ indicates the row-vector representing the gradient of the field $u$ computed at centroid $\vec{x}_j$.
Furthermore, we recall that $J$ is a scalar field exactly as in the case reported in previous subsection, thus $J \colon \R^{N_I \times L} \to \R$, where $N_I$ is the number of RBF-FD nodes that lie inside the physical domain and $L$ is the number of design parameters that we are able to control; for this specific problem design parameters $\vec{l}$ are the heights of the vertices of the triangles that define surface $A$. Finally we denote with $N_C$ the number of stl triangles that made up surface $A$, thus $\abs{\mathcal{T}} = N_C$. Even in this case the design optimization problem obtained assumes the same form of the one presented in the $1$D case, reported in equation~\eqref{eqn:design_opt_problem_with_RBF-FD_model_1D}.

% Aggiungi che lo stach di questi vettori poi ti da la matrice.

\smallskip
%TODO: Specifica il fatto che dentro i vettori u_j ci possono essere anche i valori di g (nella parte RBF-FD questa parte è più esplicita con i vettori [u, g, 0])
%The row vector $\nabla \vec{u}(\vec{x}_j)^T$ can be found through a local RBF-FD approximation as:
%\begin{equation}
%	\nabla \vec{u}(\vec{x}_j)^T =
%	\begin{bmatrix}
	%		c_x^T(\vec{x}_j) \vec{u}_j  &  c_y^T(\vec{x}_j) \vec{u}_j  &  c_z^T(\vec{x}_j) \vec{u}_j
	%	\end{bmatrix}
%\end{equation}
%which require the creation of a stencil $\mathcal{X}_j$ centered at centroid $\vec{x}_j$. To do so an initial application of the RBF-FD method is required in order to have 
Before proceeding we point out how $\nabla \vec{u}(\vec{x}_j)^T$ is computed. At first one might think to obtain it using the RBF-FD method based on the $N$ nodes scattered over the physical domain as explained in subsection~\vref{subsec:RBF-FD_formulation}. However this is not the right approach, in fact by following the aforementioned procedure is possible to approximate the gradient on the RBF-FD nodes only and since the centroids of the triangles are independent from the latter another approach is required. RBF-FD method it is still a way to go since by considering the stencil $\mathcal{X}_j$ centered on $\vec{x}_j$ is possible to refer back to equation~\eqref{eqn:FD_like_discretization_of_L} and then approximate $\nabla \vec{u}(\vec{x}_j)^T$ as:
\begin{equation}
	\begin{split}
		\nabla \vec{u}(\vec{x}_j)^T & =
		\begin{bmatrix}
			\frac{\partial}{\partial x} u(\vec{x}_j)  &  \frac{\partial}{\partial y} u(\vec{x}_j)  &  \frac{\partial}{\partial z} u(\vec{x}_j)
		\end{bmatrix} \\
		& =
		\begin{bmatrix}
			\vec{c}_x^T(\vec{x}_j) \vec{u}_j  &  \vec{c}_y^T(\vec{x}_j) \vec{u}_j  &  \vec{c}_z^T(\vec{x}_j) \vec{u}_j
		\end{bmatrix}
	\end{split}
\end{equation}
where $\vec{u}_j \in \R^m$ is the vector containing the field values at the points of stencil $ \mathcal{X}_j$ while $\vec{c}_x^T(\vec{x}_j)$, $\vec{c}_y^T(\vec{x}_j)$ and $\vec{c}_z^T(\vec{x}_j)$ are the vectors composed by the first $m$ coefficients obtained from the solution of equation~\eqref{eqn:row_of_C_system} respectively when the associated differential operator $\mathcal{L}$ in the equation is $\partial / \partial x$, $\partial / \partial y$ or $\partial / \partial z$.

%After an initial application of the RBF-FD method based on $N$ nodes scattered all over the physical domain, which are, in general, unrelated to the triangular centroids, the row vector $\nabla \vec{u}(\vec{x}_j)^T$ can be found through another local RBF-FD approximation as:
%\begin{equation}
%	\nabla \vec{u}(\vec{x}_j)^T =
%	\begin{bmatrix}
	%		c_x^T(\vec{x}_j) \vec{u}_j  &  c_y^T(\vec{x}_j) \vec{u}_j  &  c_z^T(\vec{x}_j) \vec{u}_j
	%	\end{bmatrix}
%\end{equation}
%which require the creation of a stencil $\mathcal{X}_j$ centered at $\vec{x}_j$. The $m$ neighboring nodes that constitute it are taken from those used in the first RBF-FD application. $\vec{u}_j \in \R^m$ is the vector with the values of the field $u$ at the nodes of the stencil and $c_x^T(\vec{x}_j)$, $c_y^T(\vec{x}_j)$ and $c_z^T(\vec{x}_j)$ are the vectors composed by the first $m$ coefficients obtained from the solution of equation~\eqref{eqn:row_of_C_system_in_RBF-HFD} respectively when the associated differential operator $\mathcal{L}$ in the equation is $\partial / \partial x$, $\partial / \partial y$ or $\partial / \partial z$.

%TODO: magari specifica che c_x non è esattamente lo stesso c che abbiamo trovato nel capitolo RBF-FD, qui lostencil contiene anche il punto del vertice del triangolo
%TODO: aggiungi 2 parole sul problema di ottenere nablau(xj) dato che xj non appartiene ai nodi RBF-FD è richijesto un altro giro di RBF-FD (da chiarire questo passaggio)
%TODO:     cioè sottolinea il fatto che un giro di global RBF-FD è necessario
Moving forward, if we further concatenate along rows the vectors $\nabla \vec{u}(\vec{x}_j)^T$ associated to each triangle we would obtain the matrix:
\begin{equation}
	\label{eqn:field_gradient_at_centroids}
	\nabla \vec{u} =
	\begin{bmatrix}
		\vec{c}_x^T(\vec{x}_1) \vec{u}_1  			  &  \vec{c}_y^T(\vec{x}_1) \vec{u}_1  		   &  \vec{c}_z^T(\vec{x}_1) \vec{u}_1  							\\
		\vdots									  & \vdots					   		 		   & \vdots															\\
		\vec{c}_x^T(\vec{x}_{N_C}) \vec{u}_{N_C}  &  \vec{c}_y^T(\vec{x}_{N_C}) \vec{u}_{N_C}  &  \vec{c}_z^T(\vec{x}_{N_C}) \vec{u}_{N_C}
	\end{bmatrix}
\end{equation}
which belongs to $\R^{N_C \times 3}$ and can be rewritten more compactly as:
\begin{equation}
	\nabla \vec{u} =
	\begin{bmatrix}
		\vec{C}_x \vec{u}  &  \vec{C}_y \vec{u}  &  \vec{C}_z \vec{u}
	\end{bmatrix}
\end{equation}
where $\vec{u} \in \R^{N}$ is the vector composed by the field values on the $N$ RBF-FD nodes and all the matrices $\vec{C}_x$, $\vec{C}_y$ and $\vec{C}_z$ are sparse rectangular matrices with $N_C$ rows and $N$ columns; the $i$-th row of matrix $\vec{C}_x$ is formed by the elements of the vector $\vec{c}_x^T(\vec{x}_i)$ present in equation~\eqref{eqn:field_gradient_at_centroids} and in similar way the $i$-th rows of matrices $\vec{C}_y$ and $\vec{C}_z$ are also structured, where the elements of vectors $\vec{c}_y^T(\vec{x}_i)$ and $\vec{c}_z^T(\vec{x}_i)$ are used instead of those of vector $\vec{c}_x^T(\vec{x}_i)$.

By defining the following three vectors:
\begin{equation}
	\vec{A}_x =
	\begin{bmatrix}
		a_{1,x}   \\
		\vdots			\\
		a_{N_C,x}
	\end{bmatrix} \quad
	\vec{A}_y =
	\begin{bmatrix}
		a_{1,y}   \\
		\vdots			\\
		a_{N_C,y}
	\end{bmatrix} \quad
	\vec{A}_z =
	\begin{bmatrix}
		a_{1,z}   \\
		\vdots			\\
		a_{N_C,z}
	\end{bmatrix}
\end{equation}
where the subscripts $x$, $y$, $z$ accompanying $a_1, \dots, a_{N_C}$ are used to indicate the elements in the first, second and third position of the vectors $\vec{a}_1, \dots, \vec{a}_{N_C}$, is then possible to rewrite the cost function $J$ as:
\begin{equation}
	\label{eqn:J_3D_compact_middle_step}
	J = \frac{1}{\abs{A}} ( \vec{A}_x^T \vec{C}_x \vec{u} + \vec{A}_y^T \vec{C}_y \vec{u} + \vec{A}_z^T \vec{C}_z \vec{u})
\end{equation}
Additionally, defining the vector $\vec{k}^T = \vec{A}_x^T \vec{C}_x + \vec{A}_y^T \vec{C}_y + \vec{A}_z^T \vec{C}_z$, which belongs to $\R^{N}$, equation~\eqref{eqn:J_3D_compact_middle_step} can be rewritten in an even more compact manner as:
\begin{equation}
	\label{eqn:J_3D_compact}
	J = \frac{1}{\abs{A}} \vec{k}^T \vec{u}
\end{equation}

\smallskip
Now that we have rewritten the cost function in a more convenient way we can compute its gradient. To do so we have to differentiate equation~\eqref{eqn:J_3D_compact} with respect to each design parameter; focusing on the $i$-th parameter we obtain the following derivative:
\begin{equation}
	\label{eqn:diff_J_3D_single_param_mid_step}
	\frac{\partial J}{\partial l_i} = \biggl( \frac{\partial}{\partial l_i} \abs{A}^{-1} \biggr) \vec{k}^T \vec{u} + \frac{1}{\abs{A}} \biggl( \frac{\partial \vec{k}^T}{\partial l_i}\vec{u} + \vec{k}^T \frac{\partial \vec{u}}{\partial l_i} \biggr)
\end{equation}
which once a distinction between the field values corresponding to internal and boundary nodes, denoted respectively as $\vec{u}_I \in \R^{N_I}$ and $\vec{u}_B \in \R^{N_B}$, is made, can be rewritten as:
\begin{equation}
	\frac{\partial J}{\partial l_i} = \biggl( \frac{\partial}{\partial l_i} \abs{A}^{-1} \biggr) \vec{k}^T \vec{u} + \frac{1}{\abs{A}} \biggl( \frac{\partial \vec{k}^T}{\partial l_i}\vec{u} + \vec{k}_I^T \frac{\partial \vec{u}_I}{\partial l_i} + \vec{k}_B^T \frac{\partial \vec{u}_B}{\partial l_i}\biggr)
\end{equation}
where $\vec{k}_I^T$ and $\vec{k}_B^T$ are the vectors made up of the elements of $\vec{k}^T$ associated respectively to $\vec{u}_I$ and $\vec{u}_B$.

The term $\partial\vec{u}_I/\partial l_i$ can be found in a similar way as done in the previous subsection in $1$D case: differentiating the RBF-FD constraint reported in equation~\eqref{eqn:compact_discretized_PDE}. Doing so we obtain again:
\begin{equation}
	\frac{d\vec{C}_I}{d\vec{l}} \vec{u}_I + \vec{C}_I \frac{d\vec{u}_I}{d\vec{l}} + \frac{d\vec{C}_B}{d\vec{l}} \vec{u}_B + \vec{C}_B \frac{d\vec{u}_B}{d\vec{l}} =
	\frac{d\vec{f}}{d\vec{l}}
\end{equation}
which once replaced in equation~\eqref{eqn:diff_J_3D_single_param_mid_step} gives:
\begin{multline}
	\label{eqn:diff_J_3D_single_param_before_global_adjoint}
	\frac{\partial J}{\partial l_i} = \biggl( \frac{\partial}{\partial l_i} \abs{A}^{-1} \biggr) \vec{k}^T \vec{u} \\ + \frac{1}{\abs{A}} \biggl[ \frac{\partial \vec{k}^T}{\partial l_i}\vec{u} + \vec{k}_B^T \frac{\partial \vec{u}_B}{\partial l_i} - \vec{k}_I^T \vec{C}_I^{-1} \biggl( \frac{\partial \vec{C}}{\partial l_i}\vec{u} + \vec{C}_B\frac{\partial \vec{u}_B}{\partial l_i}\biggr) \biggr]
\end{multline}
where matrix $\partial \vec{C} / \partial l_i$ is obtained by stacking vertically the matrices $\partial \vec{C}_I / \partial l_i$ and $\partial \vec{C}_B / \partial l_i$ whereas vector $\vec{u}$ is given by doing the same but with vectors $\vec{u}_I$ and $\vec{u}_B$.
Then is possible to apply the adjoint method to compute the product $\vec{k}_I^T \vec{C}_I^{-1}$ which makes equation~\eqref{eqn:diff_J_3D_single_param_before_global_adjoint}:
\begin{multline}
	\label{eqn:diff_J_3D_single_param_after_global_adjoint}
	\frac{\partial J}{\partial l_i} = \biggl( \frac{\partial}{\partial l_i} \abs{A}^{-1} \biggr) \vec{k}^T \vec{u} \\ + \frac{1}{\abs{A}} \Biggl[ \frac{\partial \vec{k}^T}{\partial l_i}\vec{u} + \vec{k}_B^T \frac{\partial \vec{u}_B}{\partial l_i} - \vec{\lambda}_1^T \biggl( \frac{\partial \vec{C}}{\partial l_i}\vec{u} + \vec{C}_B\frac{\partial \vec{u}_B}{\partial l_i}\biggr) \Biggr]
\end{multline}
where $\vec{\lambda}_1$ is found by solving:
\begin{equation}
	\vec{C}_I^T \vec{\lambda}_1 = \vec{k}_I
\end{equation}
and remain the same even when the derivative is computed respect a parameter different than $l_i$.

%TODO: Aggiungi discorso su cosa rappresentano i 2 termini nell'equazione della sensibilità di J
%TODO: Aggiungi cenno a come calcolare il primo termine nella parentesi tonda della sensibiiltà di J (aka d/dli 1/A)
%TODO: Fixa la dimensione delle parentesi quadre falle un po' più grandi

Now what remains to be computed are the $3$ unknown terms that appear within the square brackets of equation~\eqref{eqn:diff_J_3D_single_param_after_global_adjoint}. These are: $\frac{\partial \vec{k}^T}{\partial l_i}\vec{u}$, $\vec{k}_B^T \frac{\partial \vec{u}_B}{\partial l_i}$ and $\Bigl( \frac{\partial \vec{C}}{\partial l_i}\vec{u} + \vec{C}_B\frac{\partial \vec{u}_B}{\partial l_i} \Bigr)$.

\medskip
With regard to the term $\frac{\partial \vec{k}^T}{\partial l_i}\vec{u}$, we can take advantage of the equivalence between equations~\eqref{eqn:J_3D_compact_middle_step} and~\eqref{eqn:J_3D_compact} to write:
\begin{equation}
	\begin{split}
		\frac{\partial \vec{k}^T}{\partial l_i}\vec{u} & = \frac{\partial}{\partial l_i} (\vec{A}_x^T\vec{C}_x + \vec{A}_y^T\vec{C}_y + \vec{A}_z^T\vec{C}_z) \vec{u}  \\
		& = \underbrace{\biggl( \frac{\partial \vec{A}_x^T}{\partial l_i}\vec{C}_x + \frac{\partial \vec{A}_y^T}{\partial l_i}\vec{C}_y + \frac{\partial \vec{A}_z^T}{\partial l_i}\vec{C}_z \biggr) \vec{u}}_{s_1}  \\
		&\qquad + \underbrace{\biggl( \vec{A}_x^T\frac{\partial \vec{C}_x}{\partial l_i} + \vec{A}_y^T\frac{\partial \vec{C}_y}{\partial l_i} + \vec{A}_z^T\frac{\partial \vec{C}_z}{\partial l_i}\biggr) \vec{u}}_{s_2}
	\end{split}
\end{equation}
Therefore we only need to find the values of $s_1$ and $s_2$.
In the $s_1$ case, matrices $\vec{C}_x$, $\vec{C}_y$, $\vec{C}_z$ and the vector $\vec{u}$ are already known from the application of the initial global RBF-FD method, whereas the row vectors $\partial \vec{A}_x^T / \partial l_i$, $\partial \vec{A}_y^T / \partial l_i$ and $\partial \vec{A}_z^T / \partial l_i$ can be computed via automatic differentiation since they contain only geometrical information.
Alternatively, $s_1$ can be computed by accumulating the contributions $s_{1,j}$ of each sensitivity of the cost function with respect to each design parameter (i.e. triangle), which is given by
\begin{equation}
	s_{1,j} = \nabla\vec{u}(\vec{x_j})^T \frac{\partial \vec{a}_j}{\partial l_i}	
\end{equation}
where $\partial \vec{a}_j / \partial l_i$ can be computed by concatenating the elements in position $j$ of vectors $\partial \vec{A}_x^T / \partial l_i$, $\partial \vec{A}_y^T / \partial l_i$ and $\partial \vec{A}_z^T / \partial l_i$.

On the other hand, $s_2$, can be computed by applying the adjoint method to the local RBF-FD systems. Following a similar approach to what was done for $s_1$, the contribution given by a single triangle to $s_2$ is given by:
\begin{equation}
	\label{eqn:t2j_in_3D_adjoint}
	s_{2,j} = a_{j,x}\frac{\partial \vec{c}_x^T(\vec{x}_j)}{\partial l_i}\vec{u}_j + a_{j,y}\frac{\partial \vec{c}_y^T(\vec{x}_j)}{\partial l_i}\vec{u}_j + a_{j,z}\frac{\partial \vec{c}_z^T(\vec{x}_j)}{\partial l_i}\vec{u}_j
\end{equation}
Now we recall that $\vec{c}_x^T(\vec{x}_j)$ is found as solution of the following local RBF-FD system:
\begin{equation}
	\label{eqn:local_RBF-FD_system_adjoint_3D}
	\vec{M}_{BC,j}^T \vec{c}_x(\vec{x}_j) =
	\begin{bmatrix}
		\mathcal{L} \Phi(\vec{x}_j, \mathcal{X}_{j,I})  \\
		\mathcal{L} \vec{p}(\vec{x}_j)
	\end{bmatrix}
\end{equation}
which means that $\partial \vec{c}_x(\vec{x}_j) / \partial l_i$ can be found differentiating the equation above with respect to the $i$-th design parameter, resulting in:
\begin{equation}
	\label{eqn:local_RBF-FD_system_differentited}
	\frac{\partial \vec{c}_x(\vec{x}_j)}{\partial l_i} = \vec{M}_{BC,j}^{-T} \biggl( \frac{\partial \vec{h}_{j,x}}{\partial l_i} - \frac{\partial \vec{M}_{BC,j}^T}{\partial l_i}\vec{c}_x(\vec{x}_j) \biggr)
\end{equation}
where we have used the notation $\vec{h}_{j,x}$ to indicate the right-hand side of equation~\eqref{eqn:local_RBF-FD_system_adjoint_3D}.
Substituting what we have just found into the equation~\eqref{eqn:t2j_in_3D_adjoint}, rearranging the result and then applying the adjoint method finally yields:
\begin{multline}
	s_{2,j} = \biggl( a_{j,x} \frac{\partial \vec{h}_{j,x}^T}{\partial l_i} + a_{j,y} \frac{\partial \vec{h}_{j,y}^T}{\partial l_i} + a_{j,z} \frac{\partial \vec{h}_{j,z}^T}{\partial l_i} \biggr) \vec{\lambda_{2,j}}  \\
	- \biggl( a_{j,x}\vec{c}_x^T(\vec{x}_j) + a_{j,y}\vec{c}_y^T(\vec{x}_j) + a_{j,z}\vec{c}_z^T(\vec{x}_j) \biggr) \frac{\partial \vec{M}_{BC,j}}{\partial l_i} \vec{\lambda_{2,j}}
\end{multline}
where $\vec{\lambda_{2,j}} \in \R^{m+M}$ is found by solving the adjoint system:
\begin{equation}
	\label{eqn:adjoint_system_t2j}
	\vec{M}_{BC,j} \vec{\lambda_{2,j}} =
	\begin{bmatrix}
		\vec{u}_j  \\
		\vec{0}
	\end{bmatrix}
\end{equation}
once, irrespective of the number $L$ of design parameters.

\medskip
The term $\vec{k}_B^T \frac{\partial \vec{u}_B}{\partial l_i}$ does not require any particular care except for the computation of $\frac{\partial \vec{u}_B}{\partial l_i}$ whose elements are found by applying the RBF-FD procedure. On the other hand, $\vec{k}_B^T$, can be found from its definition since $\vec{A}_x$, $\vec{A}_y$, $\vec{A}_z$ and $\vec{C}_x$, $\vec{C}_y$, $\vec{C}_z$ are known.

\medskip
The last term that remain to be computed, $\vec{\lambda}_1^T \Bigl( \frac{\partial \vec{C}}{\partial l_i}\vec{u} + \vec{C}_B\frac{\partial \vec{u}_B}{\partial l_i} \Bigr)$, can be divided as follow:
\begin{equation}
	\vec{\lambda}_1^T \biggl( \frac{\partial \vec{C}}{\partial l_i}\vec{u} + \vec{C}_B\frac{\partial \vec{u}_B}{\partial l_i} \biggr) = \underbrace{\vec{\lambda}_1^T \frac{\partial \vec{C}}{\partial l_i} \vec{u}}_{t_1} + \underbrace{\vec{\lambda}_1^T \vec{C}_B \frac{\partial \vec{u}_B}{\partial l_i}}_{t_2}
\end{equation}

$t_2$ is composed by already known terms: $\vec{C}_B$ is found from the initial global RBF-FD application while the other two terms that comprise it have already been calculated during the previous steps; we do not dwell too much on it. A term that deserve more attention is $t_1$ since it requires to differentiate matrix $\vec{C}$ which is built row by row analyzing the stencil associated to each RBF-FD node.
The contribution $t_{1,k}$ given by the $k$-th RBF-FD node to $t_1$ is found as:
\begin{equation}
	\label{eqn:t1k_in_3D_adjoint}
	t_{1,k} = \lambda_{1,k} \frac{\partial \vec{c}^T(\vec{x}_K)}{\partial l_i} \vec{u}_k
\end{equation}
This makes clearer the relationship with the adjoint method applied in 1D case: the last two terms in equation~\eqref{eqn:t1k_in_3D_adjoint} made up the element $q_{k,i}$ of the matrix $\vec{Q}$ defined in equation~\eqref{eqn:Q_matrix_definition} of subsection~\ref{subsec:adjoint_method_RBF-FD_1D}. Thus repeating the passages reported in equations~(\ref{eqn:local_RBF-FD_system_adjoint_1D} -~\ref{eqn:Q_elems_before_adjoint}) we found that $t_{1,k}$ can be rewritten as:
\begin{equation}
	t_{1,k} = \lambda_{1,k} \biggl( \frac{\partial \vec{h}}{\partial l_i} - \frac{\partial \vec{M}_{BC,k}^T}{\partial l_i} \vec{c}(\vec{x}_k) \biggr)^T \vec{M}_{BC,k}^{-1} \vec{u}_k
\end{equation}
and subsequently, by means of the adjoint vector $\vec{\lambda_{2,k}}$ as:
\begin{equation}
	t_{1,k} = \lambda_{1,k} \biggl( \frac{\partial \vec{h}}{\partial l_i} - \frac{\partial \vec{M}_{BC,k}^T}{\partial l_i} \vec{c}(\vec{x}_k) \biggr)^T \vec{\lambda}_{2,k}
\end{equation}
where $\vec{\lambda_{2,k}}$ is found by solving the system in~\eqref{eqn:adjoint_system_t2j} based on $k$-th, instead of the $j$-th, stencil.

\smallskip
What we have just shown is the application of the adjoint method to the optimization of a $3$D problem based on an RBF-FD model, but the reasoning, with small changes, can be also applied similarly to problems which are based on RBF-HFD models as has been done in the $1$D case.

%To efficiently calculate $t_1$, it is possible to use the adjoint method similarly to what has been done in the $1$D case: below we report the steps. We initially note that the contribution $t_{1,k}$ to $t_1$ given by the $k$-th RBF-FD node is given by:
%\begin{equation}
%	\label{eqn:t1k_in_3D_adjoint}
%	t_{1,k} = \vec{\lambda}_{1,k} \frac{\partial \vec{c}^T(\vec{x}_k)}{\partial l_i} \vec{u}_k
%\end{equation}
%This makes clearer the relationship with the adjoint method applied in 1D case: the last two terms in equation~\eqref{eqn:t1k_in_3D_adjoint} made up the element in position 
%The vector $\frac{\partial \vec{c}^T(\vec{x}_k)}{\partial l_i}$ is found differentiating the global RBF-FD constraint repeating the passages in a similar way as reported in equations]\eqref{eqn:local_RBF-FD_system_adjoint_3D} and~\eqref{eqn:local_RBF-FD_system_differentited} and once it is substituted in~\eqref{eqn:t1k_in_3D_adjoint} yields:
%\begin{equation}
%	
%\end{equation}



%TODO: Questa cosa del perché è possibile è meglio se la spieghi dopo aver spiegato tutti i passaggi o dopo essere arrivato alla ri-definizione  di q
%this is indeed possible since $\frac{\partial \vec{C}}{\partial l_i} \vec{u}$ is nothing more than a row of the matrix $\vec{Q}$ defined in~\eqref{eqn:Q}

%The description of the cost function obtained in equation~\eqref{eqn:J_3D_compact} is 
%Now that we have described $J$ in a way that is easier to handle we can differentiate it respect a generic design parameter. . The gradient can be

%we are eventually allowed to rewrite the cost function as 

%If we concatenate by rows each $\nabla \vec{u}(\vec{x}_j)^T$ of the surface triangles we obtain a matrix $\nabla \vec{u} \in \R^{N_C \times 3}$ such that $N_C = \abs*{\mathcal{T}}$.
%Combining the aforementioned rows we would obtain a matrix
%is a row of the matrix $\nabla \vec{u} \in \R^{N_C \times 3}$ and consists on
%The matrix is obtained by means of an RBF-FD approximation
%$\nabla \vec{u} \in \R^{N \times 3}$ is a matrix whose $j$-th row, which we denote by $\nabla \vec{u}(\vec{x}_j)^T$, consists of the gradient of the field $u$ computed at point $\vec{x}_j$.
%The aforementioned matrix $\nabla \vec{u}$ is obtained by means of three RBF-FD approximation
%\begin{equation}
%	\nabla \vec{u} = 
%\end{equation}


%By aacrivionmg om some subset of he traingle sof the domain is possible to modify the shape of the geometry nevertheless it is too hard for meIts shape can be modified by acting only on the heights of the vertices of some 
%
%Each triangle of the mesh will have its own vertices which occupy a point $\vec{x} \in\R^3$, by Its shape can be modified by acting on the height of a subset of vertices of the geometry


%\section{Application to RBF-FD}
%
%In section~\vref{sec:RBF-FD} we have seen how the RBF-FD solver for Partial Differential Equations (PDEs) is implemented. Computational Fluid Dynamics (CFD), where PDEs are employed to model fluid flow and heat transfer problems, is one of the main areas where the application of the RBF-FD method could bring great benefits.
%CFD is one of the main engineering application where design optimization problems arise almost naturally. However the computational cost of the optimization in the analysis phase is very high it would be of interest apply the adjoint method in those problems
%
%Once a generic description for the design optimization problem has been written it has to be translated into a mathematical statement in order to solve it through an optimization algorithm. To do so the following items are used for a correct formulation:
%\begin{itemize}
%	\item \emph{Design variables} represented by a vector $\vec{l} = \left[ l_1, \dots, l_L \right] \in \R^L$. We consider only continuous variables since they control the shapes of objects which are allowed to vary continuously in the space;
%	
%	\item \emph{Objective function} indicated with $J \colon \R^{N_I}\times\R^L \to \R$. It is a scalar function that yields a quantity used to determines if one design is better than another. It is an explicit function of the state $\vec{u}_I$ of the problem, which is obtained from the RBF-FD solver, and the design parameters $\vec{l}$. Its choice is crucial: it has to represent the real goal of the whole optimization. A poor choice would lead to a mathematical optimum which does not match with the engineering optimum;
%	
%	\item \emph{Constraints} represented by the RBF-FD governing equations: these are reported in equation~\eqref{eqn:discretized_version_of_PDE_using_RBF-FD} in subsection~\ref{subsec:RBF-FD_formulation}. We reiterate them here, in a more compact form, since they will be used extensively throughout the rest of this section:
%	\begin{equation}
%		\label{eqn:compact_discretized_PDE}
%		\vec{C}_I \vec{u}_I + \vec{C}_B \vec{u}_B = \vec{f}
%	\end{equation}
%	where we have used the following notation: $\vec{u}_I = \left[ u(\vec{x}_1), \dots u(\vec{x}_{N_I}) \right]$, $\vec{u}_B = \left[ g(\vec{x}_{N_I+1}), \dots, g(\vec{x}_N) \right]$ and $\vec{f} = \left[ f(\vec{x}_1), \dots, f(\vec{x}_{N_I}) \right]$, recalling that $N$ is the number of RBF-FD nodes scattered throughout the physical domain $\Omega\cup\partial\Omega$ of the system which is represented by the volume of the object. The first $N_I$ nodes are contained within it, the remaining ones are placed on its surface.
%	Before proceeding it is worth noting that, in general, matrices $\vec{C}_I$ and $\vec{C}_B$, and vectors $\vec{f}$ and $\vec{u}_B$ depend on design variables $\vec{l}$. Therefore the solution $\vec{u}_I$ will also be a function (indirectly through constraint~\eqref{eqn:compact_discretized_PDE}) of design variables.
%\end{itemize}
%Therefore the resulting optimization problem can be written as:
%\begin{equation}
%	\label{eqn:RBF-FD_opt_problem}
%	\begin{aligned}
%		\text{minimize} & \quad J(\vec{u}_I, \vec{l})								\\
%		\text{by varying} & \quad \vec{u}_I											\\
%						& \quad \vec{l} 											\\
%		\text{subject to} & \quad \vec{C}_I \vec{u}_I + \vec{C}_B \vec{u}_B = \vec{f}					
%	\end{aligned}
%\end{equation}
%which optimum is found through the gradient approach explained in section~\ref{sec:application_to_design_opt}. In general with a similar formulation can be solved also problems which require the maximization of the objective function since $\max \left[ J(\vec{u}_I, \vec{l}) \right] = - \min \left[ - J(\vec{u}_I, \vec{l}) \right]$. Finally we remark that the variables that can be modified at each optimization cycle are only the design variables $\vec{l}$ since the result of CFD simulation $\vec{u}_I$ is fully determined from constraints~\eqref{eqn:compact_discretized_PDE};
%
%In the two following subsections we will explain how the gradient $\frac{dJ}{d\vec{l}}$ can be computed efficiently by means of the adjoint method for the optimization problem~\eqref{eqn:RBF-FD_opt_problem} in case of one and three-dimensional objects.
























